<<<<<<< HEAD
import sys
import os
from pathlib import Path
import itertools  # D√πng cho Grid Search

# --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ---
CURRENT_FILE = Path(__file__).resolve()
PROJECT_ROOT = CURRENT_FILE.parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

import torch
import torch.nn.functional as F
from torch_geometric.loader import LinkNeighborLoader
from torch_geometric.nn import to_hetero
from torch_geometric.transforms import RandomLinkSplit
from sklearn.metrics import roc_auc_score
from tqdm import tqdm
import pickle
import argparse
import numpy as np

from config.settings import (
    GRAPH_PATH, MODEL_PATH, PYG_DATA_PATH, MAPPING_PATH,
    INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, EPOCHS, BATCH_SIZE, LEARNING_RATE
)
from infrastructure.repositories.graph_repo import PickleGraphRepository
from core.ai.gnn_architecture import GraphSAGE
from core.ai.data_processor import GraphDataProcessor
from infrastructure.repositories.feature_repo import PyGDataRepository


# --- 1. CHU·∫®N B·ªä D·ªÆ LI·ªÜU ---
def get_or_prepare_data():
    """T·∫£i ho·∫∑c t·∫°o m·ªõi d·ªØ li·ªáu PyG."""
    feature_repo = PyGDataRepository(PYG_DATA_PATH, MAPPING_PATH)
    data, mapping = feature_repo.load_data()

    if data is None:
        print("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu PyG. ƒêang x·ª≠ l√Ω t·ª´ NetworkX...")
        repo = PickleGraphRepository(GRAPH_PATH)
        G = repo.load_graph()
        if G is None:
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y ƒë·ªì th·ªã t·∫°i {GRAPH_PATH}")

        processor = GraphDataProcessor()
        data, mapping = processor.process_graph_to_pyg(G)
        feature_repo.save_data(data, mapping)

    return data



# --- 2. C√ÅC H√ÄM HU·∫§N LUY·ªÜN & ƒê√ÅNH GI√Å ---

def train_epoch(model, loader, optimizer, device, target_edge_type):
    """Ch·∫°y 1 epoch hu·∫•n luy·ªán."""
    model.train()
    total_loss = 0
    total_examples = 0

    for batch in tqdm(loader, desc="Training", leave=False):
        batch = batch.to(device)
        optimizer.zero_grad()

        # Forward
        z_dict = model(batch.x_dict, batch.edge_index_dict)

        # L·∫•y nh√£n v√† index c·∫°nh c·∫ßn d·ª± ƒëo√°n trong batch n√†y
        edge_label_index = batch[target_edge_type].edge_label_index
        edge_label = batch[target_edge_type].edge_label

        # Decode (T√≠nh ƒëi·ªÉm)
        src_type, _, dst_type = target_edge_type
        z_src = z_dict[src_type][edge_label_index[0]]
        z_dst = z_dict[dst_type][edge_label_index[1]]
        out = (z_src * z_dst).sum(dim=-1)

        # Loss
        loss = F.binary_cross_entropy_with_logits(out, edge_label)
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * edge_label.size(0)
        total_examples += edge_label.size(0)

    return total_loss / total_examples


@torch.no_grad()
def evaluate(model, loader, device, target_edge_type):
    """ƒê√°nh gi√° m√¥ h√¨nh (t√≠nh AUC)."""
    model.eval()
    preds = []
    ground_truths = []

    for batch in tqdm(loader, desc="Evaluating", leave=False):
        batch = batch.to(device)
        z_dict = model(batch.x_dict, batch.edge_index_dict)

        edge_label_index = batch[target_edge_type].edge_label_index
        edge_label = batch[target_edge_type].edge_label

        src_type, _, dst_type = target_edge_type
        z_src = z_dict[src_type][edge_label_index[0]]
        z_dst = z_dict[dst_type][edge_label_index[1]]

        out = (z_src * z_dst).sum(dim=-1).sigmoid()

        preds.append(out.cpu().numpy())
        ground_truths.append(edge_label.cpu().numpy())

    return roc_auc_score(np.concatenate(ground_truths), np.concatenate(preds))


# --- 3. CHI·∫æN L∆Ø·ª¢C CH·∫†Y ---

def train_one_config(data, config, device, target_edge_type, final_mode=False):
    """
    Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi 1 b·ªô tham s·ªë c·ª• th·ªÉ.
    """
    # 1. KH·ªûI T·∫†O T·ª™ ƒêI·ªÇN L·ªäCH S·ª¨
    history = {
        "epoch": [],
        "loss": [],
        "val_auc": []  # C√≥ th·ªÉ r·ªóng n·∫øu l√† final_mode
    }
    hidden_dim = config['hidden_dim']
    lr = config['lr']
    epochs = config['epochs']

    print(f"\n‚öôÔ∏è C·∫•u h√¨nh: Hidden={hidden_dim}, LR={lr}")

    # 1. Chia d·ªØ li·ªáu (n·∫øu kh√¥ng ph·∫£i final)
    if final_mode:
        train_data = data
        val_loader = None
    else:
        # RandomLinkSplit ƒë·ªÉ t·∫°o t·∫≠p Train/Val/Test
        transform = RandomLinkSplit(
            num_val=0.1,
            num_test=0.1,
            is_undirected=True,
            add_negative_train_samples=False,
            edge_types=[target_edge_type]
        )
        train_data, val_data, test_data = transform(data)

        # Loader cho t·∫≠p Validation
        val_loader = LinkNeighborLoader(
            val_data,
            num_neighbors=[10, 5],
            edge_label_index=(target_edge_type, val_data[target_edge_type].edge_label_index),
            edge_label=val_data[target_edge_type].edge_label,
            batch_size=BATCH_SIZE,  # D√πng batch size l·ªõn h∆°n cho eval c≈©ng ƒë∆∞·ª£c
            shuffle=False
        )

    # 2. Loader cho t·∫≠p Train
    # (Quan tr·ªçng: LinkNeighborLoader gi√∫p kh√¥ng tr√†n RAM)
    train_loader = LinkNeighborLoader(
        train_data,
        num_neighbors=[10, 5],
        edge_label_index=(target_edge_type, train_data[target_edge_type].edge_index),
        neg_sampling_ratio=1.0,
        batch_size=BATCH_SIZE,
        shuffle=True
    )

    # 3. Model & Optimizer
    base_model = GraphSAGE(hidden_channels=hidden_dim, out_channels=OUTPUT_DIM, in_channels=INPUT_DIM)
    model = to_hetero(base_model, data.metadata(), aggr='sum').to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    best_val_auc = 0
    best_model_state = None

    # 4. Loop
    for epoch in range(1, epochs + 1):

        loss = train_epoch(model, train_loader, optimizer, device, target_edge_type)
        history["epoch"].append(epoch)
        history["loss"].append(float(loss))  # √âp ki·ªÉu float ƒë·ªÉ tr√°nh l·ªói JSON
        log_msg = f"Epoch {epoch:03d} | Loss: {loss:.4f}"

        # N·∫øu c√≥ t·∫≠p Val -> ƒê√°nh gi√° & L∆∞u Best Model
        if val_loader:
            val_auc = evaluate(model, val_loader, device, target_edge_type)
            history["val_auc"].append(float(val_auc))

            log_msg += f" | Val AUC: {val_auc:.4f}"

            if val_auc > best_val_auc:
                best_val_auc = val_auc
                best_model_state = model.state_dict()

        print(log_msg)
    if final_mode:
        print(f"üíæ ƒêang l∆∞u l·ªãch s·ª≠ hu·∫•n luy·ªán v√†o {TRAINING_HISTORY_PATH}...")
        try:
            with open(TRAINING_HISTORY_PATH, "w", encoding="utf-8") as f:
                json.dump(history, f, indent=4)
            print("‚úÖ ƒê√£ l∆∞u l·ªãch s·ª≠ th√†nh c√¥ng!")
        except Exception as e:
            print(f"‚ùå L·ªói khi l∆∞u l·ªãch s·ª≠: {e}")
    # N·∫øu Final Mode (kh√¥ng c√≥ Val), l·∫•y state cu·ªëi c√πng
    if final_mode:
        best_model_state = model.state_dict()
        best_val_auc = 1.0  # (Gi·∫£ ƒë·ªãnh)

    return best_val_auc, best_model_state


def run_grid_search():
    """Ch·∫°y t√¨m ki·∫øm tham s·ªë t·ªëi ∆∞u."""
    data = get_or_prepare_data()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    target_edge_type = ('person','knows','person')

    # ƒê·ªãnh nghƒ©a l∆∞·ªõi tham s·ªë
    param_grid = {
        'hidden_dim': [64, 128],
        'lr': [0.01,0.001],
        'epochs': [20]  # Test nhanh 20 epoch
    }

    keys, values = zip(*param_grid.items())
    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]

    best_auc = 0
    best_params = None

    print(f"üöÄ B·∫Øt ƒë·∫ßu Grid Search tr√™n {len(combinations)} c·∫•u h√¨nh...")

    for config in combinations:
        auc, _ = train_one_config(data, config, device, target_edge_type)

        if auc > best_auc:
            best_auc = auc
            best_params = config
            print(f"üèÜ K·ª∑ l·ª•c m·ªõi: AUC {auc:.4f} v·ªõi {config}")

    print(f"\n‚úÖ Grid Search Ho√†n t·∫•t. T·ªët nh·∫•t: {best_params} (AUC: {best_auc:.4f})")

    # Sau khi t√¨m ƒë∆∞·ª£c, ch·∫°y Final Training v·ªõi tham s·ªë t·ªët nh·∫•t
    print("\nüèãÔ∏è B·∫Øt ƒë·∫ßu Final Training (100 Epochs) v·ªõi tham s·ªë t·ªët nh·∫•t...")
    best_params['epochs'] = 100  # Train k·ªπ
    _, final_state = train_one_config(data, best_params, device, target_edge_type, final_mode=True)

    # L∆∞u Model cu·ªëi c√πng
    print(f"üíæ ƒêang l∆∞u Final Model v√†o {MODEL_PATH}...")
    torch.save(final_state, MODEL_PATH)


if __name__ == "__main__":
    run_grid_search()
=======
import torch
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv, to_hetero
from torch_geometric.data import HeteroData
from torch_geometric.transforms import ToUndirected
import torch_geometric.transforms as T
from torch_geometric.utils import sort_edge_index
from torch_geometric.loader import LinkNeighborLoader
import itertools
import json
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
from sklearn.metrics import roc_auc_score
from infrastructure.repositories import PyGDataRepository
from config.settings import  PYG_DATA_PATH, OUTPUT_DIM, TRAINING_HISTORY_PATH, MODEL_PATH, \
    PYG_TRAINING_DATA_PATH
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
torch.set_float32_matmul_precision('medium')
from torch.cuda.amp import GradScaler, autocast


# Architecture
class GraphSAGE(torch.nn.Module):
    def __init__(self,hidden_channels, out_channels, dropout):
        super().__init__()
        self.conv1 = SAGEConv((-1,-1), hidden_channels)
        self.ln1 = torch.nn.LayerNorm(hidden_channels)
        self.conv2 = SAGEConv((-1,-1), out_channels)
        self.dropout = torch.nn.Dropout(p=dropout)

    def forward(self, x, edge_index):

        x = self.conv1(x, edge_index)
        x = self.ln1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.conv2(x, edge_index)
        x = F.normalize(x, p=2., dim=-1)
        return x

class InteractionMLP(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, dropout):
        super().__init__()
        # Input dim * 3 v√¨ ta s·∫Ω n·ªëi [src, dst, src*dst]
        self.lin1 = torch.nn.Linear(input_dim * 3, hidden_dim)
        self.lin2 = torch.nn.Linear(hidden_dim, hidden_dim // 2)
        self.lin3 = torch.nn.Linear(hidden_dim // 2, output_dim)

        self.dropout = torch.nn.Dropout(p=dropout)

    def forward(self, z_src, z_dst):
        # T·∫†O T∆Ø∆†NG T√ÅC M·∫†NH M·∫º
        # 1. ƒê·∫∑c tr∆∞ng g·ªëc: z_src, z_dst
        # 2. ƒê·∫∑c tr∆∞ng t∆∞∆°ng ƒë·ªìng: z_src * z_dst (Hadamard product)
        # Gi√∫p model d·ªÖ d√†ng h·ªçc ƒë∆∞·ª£c "A v√† B c√≥ gi·ªëng nhau kh√¥ng?"
        combined = torch.cat([z_src, z_dst, z_src * z_dst], dim=1)

        h = self.lin1(combined)
        h = F.relu(h)
        h = self.dropout(h)

        h = self.lin2(h)
        h = F.relu(h)
        h = self.dropout(h)

        h = self.lin3(h)

        # √âp v·ªÅ 1 chi·ªÅu ƒë·ªÉ tr√°nh l·ªói shape [N, 1] vs [N]
        return h.view(-1)
import torch
import torch.nn.functional as F
from torch_geometric.nn import HGTConv, Linear

class HGTLinkPrediction(torch.nn.Module):
    def __init__(self,  hidden_channels, out_channels,data, dropout=0.5, num_heads=4, num_layers=3):
        super().__init__()

        # 1. INPUT PROJECTION (Quan tr·ªçng)
        # Bi·∫øn ƒë·ªïi vector Text (768 dim) + Year (1 dim) v·ªÅ kh√¥ng gian chung (256 dim)
        # Gi√∫p model h·ªçc ƒë∆∞·ª£c ƒë·∫∑c tr∆∞ng ri√™ng cho b√†i to√°n n√†y
        self.lin_dict = torch.nn.ModuleDict()
        for node_type in data.node_types:
            # L·∫•y k√≠ch th∆∞·ªõc feature ƒë·∫ßu v√†o th·ª±c t·∫ø t·ª´ data
            in_dim = data[node_type].x.size(1)
            self.lin_dict[node_type] = Linear(in_dim, hidden_channels)

        # 2. HGT LAYERS (Thay cho SAGE)
        # HGT d√πng c∆° ch·∫ø Attention ƒë·ªÉ "ƒë·ªçc hi·ªÉu" feature text t·ªët h∆°n SAGE
        self.convs = torch.nn.ModuleList()
        for _ in range(num_layers):
            conv = HGTConv(hidden_channels, hidden_channels, data.metadata(),
                           heads=num_heads)
            self.convs.append(conv)

        self.dropout = torch.nn.Dropout(p=dropout)

        # 3. INTERACTION DECODER (Nh∆∞ ƒë√£ b√†n)
        self.decoders = torch.nn.ModuleDict()
        for et in data.edge_types:
            src, rel, dst = et
            if rel.startswith('rev_'): continue
            key = f"{src}__{rel}__{dst}"

            # Decoder v·ªõi input l√† hidden_channels
            self.decoders[key] = InteractionMLP(hidden_channels, 64, 1, dropout)

    def forward(self, x_dict, edge_index_dict, target_edge_type, edge_label_index):
        # A. Projection: √âp feature text + year v√†o kh√¥ng gian Hidden
        dtype = self.kqv_lin.weights[0].dtype if hasattr(self.kqv_lin, 'weights') else torch.float32
        x_dict = {k: v.to(dtype) for k, v in x_dict.items()}
        x_start = {}
        for node_type, x in x_dict.items():
            x_start[node_type] = self.lin_dict[node_type](x).relu_()
            x_start[node_type] = self.dropout(x_start[node_type])

        # B. HGT Message Passing
        for conv in self.convs:
            x_start = conv(x_start, edge_index_dict)

        # C. Decode
        src_type, rel, dst_type = target_edge_type
        z_src = x_start[src_type][edge_label_index[0]]
        z_dst = x_start[dst_type][edge_label_index[1]]

        key = f"{src_type}__{rel}__{dst_type}"
        if key in self.decoders:
            return self.decoders[key](z_src, z_dst)
        else:
            return torch.zeros(z_src.size(0), device=z_src.device)

class LinkPredictionModel(torch.nn.Module):
    def __init__(self, hidden_channels, out_channels, data=None, dropout=0.5):
        super().__init__()

        # D√πng DeepGraphSAGE thay v√¨ b·∫£n th∆∞·ªùng
        self.gnn = GraphSAGE(hidden_channels, out_channels, dropout)
        self.encoder = to_hetero(self.gnn, data.metadata(), aggr='sum')

        self.decoders = torch.nn.ModuleDict()

        # Init Decoder cho t·ª´ng lo·∫°i c·∫°nh
        for et in data.edge_types:
            src, rel, dst = et
            if rel.startswith('rev_'): continue

            key = f"{src}__{rel}__{dst}"
            # Decoder m·ªõi th√¥ng minh h∆°n
            self.decoders[key] = InteractionMLP(out_channels, 64, 1, dropout)

    def forward(self, x, edge_index, target_edge_type, edge_label_index):
        z_dict = self.encoder(x, edge_index)

        src_type, rel, dst_type = target_edge_type

        z_src = z_dict[src_type][edge_label_index[0]]
        z_dst = z_dict[dst_type][edge_label_index[1]]

        key = f"{src_type}__{rel}__{dst_type}"

        if key in self.decoders:
            return self.decoders[key](z_src, z_dst)
        else:
            return torch.zeros(z_src.size(0), device=z_src.device)

# Function to train
import optuna

from config.settings import BATCH_SIZE, MODEL_PATH
from infrastructure.repositories import ModelRepository
import gc



def get_or_prepare_data():
    """T·∫£i v√† chu·∫©n b·ªã d·ªØ li·ªáu (Undirected + Sanitize)."""
    feature_repo = PyGDataRepository(PYG_DATA_PATH)
    data = feature_repo.load_data()

    if data is None:
        print("Ch∆∞a c√≥ d·ªØ li·ªáu PyG. Vui l√≤ng ch·∫°y ETL tr∆∞·ªõc!")
        return None

    # X√≥a c·∫°nh r·ªóng
    #data = sanitize_hetero_data(data)

    return data


def loader_generator(data_source, target_edge_types, batch_size, shuffle=False):
    for et in target_edge_types:
        # 1. Ki·ªÉm tra nhanh (gi·ªØ nguy√™n logic c≈© c·ªßa b·∫°n)
        if et not in data_source.edge_index_dict: pass
        if hasattr(data_source[et], 'edge_label_index') and data_source[et].edge_label_index.numel() == 0:
            continue

        # 2. Chu·∫©n b·ªã nh√£n
        lbl_index = data_source[et].edge_label_index
        lbl_ones = torch.ones(lbl_index.size(1), dtype=torch.float32)

        # 3. Kh·ªüi t·∫°o Loader (Ch·ªâ t·ªën RAM t·∫°i th·ªùi ƒëi·ªÉm n√†y)

        loader = LinkNeighborLoader(
            data_source,
            num_neighbors=[15, 10],
            edge_label_index=(et, lbl_index),
            edge_label=lbl_ones,
            neg_sampling_ratio=1.0,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=0,        # TƒÉng l√™n 4 ho·∫∑c 6 (t√πy s·ªë nh√¢n CPU c·ªßa b·∫°n)
        )
        # 4. Tr·∫£ v·ªÅ ƒë·ªÉ d√πng ngay
        yield et, loader


def train_epoch(model, data, optimizer, device, target_edge_types, scaler, batch_size=BATCH_SIZE):
    model.train()
    total_loss = 0
    total_examples = 0
    count = 1
    max = len(target_edge_types)
    data_loader_gen = loader_generator(data, target_edge_types, batch_size, shuffle=True)
    for edge_type, loader in data_loader_gen:
        pbar = tqdm(loader, desc="Training", leave=False)
        pbar.set_postfix({
            "relationship": f" {edge_type[1]} | {count}/{max}"
        })
        for batch in pbar:
            batch = batch.to(device)
            optimizer.zero_grad()

            # V·ªõi single-type loader, ta bi·∫øt ch·∫Øc ch·∫Øn c·∫°nh c·∫ßn d·ª± ƒëo√°n l√† edge_type
            # PyG t·ª± ƒë·ªông g√°n v√†o edge_label_index c·ªßa edge_type ƒë√≥ trong batch
            edge_label_index = batch[edge_type].edge_label_index
            edge_label = batch[edge_type].edge_label
            with torch.amp.autocast('cuda'):
                # Forward
                out = model(batch.x_dict, batch.edge_index_dict, edge_type, edge_label_index)

                # Loss
                loss = F.binary_cross_entropy_with_logits(out, edge_label)

            scaler.scale(loss).backward()

            # 3. Optimizer Step
            scaler.step(optimizer)
            scaler.update()  # C·∫≠p nh·∫≠t l·∫°i h·ªá s·ªë scale cho l·∫ßn sau

            total_loss += loss.item() * edge_label.size(0)
            total_examples += edge_label.size(0)
        count += 1
        del loader, pbar
        gc.collect()
    return total_loss / (total_examples + 1e-6)


@torch.no_grad()
def evaluate(model, data, device, target_edge_types, batch_size=BATCH_SIZE):
    """
    ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p Val ho·∫∑c Test.

    Args:
        model: M√¥ h√¨nh GNN ƒë√£ hu·∫•n luy·ªán.
        loaders: List c√°c LinkNeighborLoader (m·ªói loader ·ª©ng v·ªõi 1 lo·∫°i c·∫°nh).
        device: 'cuda' ho·∫∑c 'cpu'.
        target_edge_types: List c√°c lo·∫°i c·∫°nh t∆∞∆°ng ·ª©ng v·ªõi loaders.

    Returns:
        score: Ch·ªâ s·ªë ROC-AUC (0.0 -> 1.0).
    """
    model.eval()  # Chuy·ªÉn model sang ch·∫ø ƒë·ªô ƒë√°nh gi√° (t·∫Øt Dropout, kh√≥a BatchNorm)

    preds = []
    ground_truths = []

    # 1. Duy·ªát song song qua t·ª´ng c·∫∑p (Lo·∫°i c·∫°nh, Loader t∆∞∆°ng ·ª©ng)
    # L∆∞u √Ω: target_edge_types v√† loaders ph·∫£i c√≥ c√πng ƒë·ªô d√†i v√† th·ª© t·ª±
    count = 1
    max = len(target_edge_types)
    data_loader_gen = loader_generator(data, target_edge_types, batch_size, shuffle=False)
    for edge_type, loader in data_loader_gen:

        pbar = tqdm(loader, desc="Validation", leave=False)
        pbar.set_postfix({
            "relationship": f" {edge_type[1]} | {count}/{max}"
        })
        for batch in pbar:
            batch = batch.to(device)

            # Ki·ªÉm tra an to√†n: Batch c√≥ ch·ª©a nh√£n cho lo·∫°i c·∫°nh n√†y kh√¥ng?
            if not hasattr(batch[edge_type], 'edge_label_index') or batch[edge_type].edge_label_index.numel() == 0:
                continue

            # L·∫•y d·ªØ li·ªáu "ƒë·ªÅ thi"
            edge_label_index = batch[edge_type].edge_label_index
            edge_label = batch[edge_type].edge_label

            # 3. Forward Pass
            # Truy·ªÅn ƒë√∫ng edge_type ƒë·ªÉ model bi·∫øt d√πng tr·ªçng s·ªë n√†o (n·∫øu c√≥ chia t√°ch)
            # Output model th∆∞·ªùng l√† Logits (ch∆∞a qua Sigmoid)
            with torch.amp.autocast('cuda'):
                out = model(batch.x_dict, batch.edge_index_dict, edge_type, edge_label_index)
                # Sigmoid c≈©ng n√™n n·∫±m trong context n√†y (ho·∫∑c kh√¥ng, t√πy √Ω, nh∆∞ng forward model b·∫Øt bu·ªôc ph·∫£i c√≥)
                out = torch.sigmoid(out)

            # 4. L∆∞u l·∫°i k·∫øt qu·∫£ (ƒê∆∞a v·ªÅ CPU v√† Numpy ƒë·ªÉ t√≠nh to√°n b·∫±ng Sklearn)
            preds.append(out.cpu().numpy())
            ground_truths.append(edge_label.cpu().numpy())
        count += 1
        del loader, pbar
    # 5. X·ª≠ l√Ω tr∆∞·ªùng h·ª£p kh√¥ng c√≥ d·ªØ li·ªáu (tr√°nh l·ªói crash)
    if len(preds) == 0:
        print("Not data for validation.")
        return 0.0

    # 6. G·ªôp t·∫•t c·∫£ c√°c m·∫£ng numpy l·∫°i th√†nh 1 m·∫£ng d√†i duy nh·∫•t
    final_preds = np.concatenate(preds)
    final_labels = np.concatenate(ground_truths)

    if np.isnan(final_preds).any():
        print("‚ùå L·ªñI NGHI√äM TR·ªåNG: Model output ch·ª©a NaN!")
        return 0.0

    # Ki·ªÉm tra xem Labels c√≥ ƒë·ªß 2 l·ªõp (0 v√† 1) kh√¥ng
    unique_labels = np.unique(final_labels)
    if len(unique_labels) < 2:
        print(f"‚ö†Ô∏è C·∫¢NH B√ÅO: T·∫≠p Label ch·ªâ ch·ª©a 1 lo·∫°i nh√£n duy nh·∫•t: {unique_labels}")
        print("-> L√Ω do: Loader Validation ch∆∞a b·∫≠t 'neg_sampling_ratio'!")
        return 0.0

    # 7. T√≠nh ROC-AUC Score
    try:
        return roc_auc_score(final_labels, final_preds)
    except ValueError as e:
        print(f"‚ùå SKLEARN ERROR: {e}")
        # In th√™m th·ªëng k√™ ƒë·ªÉ bi·∫øt t·∫°i sao
        print(f"Min Pred: {final_preds.min()}, Max Pred: {final_preds.max()}")
        print(f"Unique Labels: {np.unique(final_labels)}")
        return 0.0


# --- 3. CHI·∫æN L∆Ø·ª¢C CH·∫†Y ---
def get_edge_pairs(data):
    """
    T·ª± ƒë·ªông b·∫Øt c·∫∑p c·∫°nh thu·∫≠n v√† c·∫°nh ngh·ªãch.
    Quy t·∫Øc: C·∫°nh ngh·ªãch c√≥ th√™m ti·ªÅn t·ªë 'rev_' ho·∫∑c l√† chi·ªÅu ng∆∞·ª£c l·∫°i.
    """
    forward_edges = []
    reverse_edges = []

    for edge_type in data.edge_types:
        src, rel, dst = edge_type

        # 1. B·ªè qua n·∫øu ƒë√¢y l√† c·∫°nh 'rev_' (ch√∫ng ta s·∫Ω x·ª≠ l√Ω n√≥ khi g·∫∑p c·∫°nh thu·∫≠n)
        if rel.startswith('rev_'):
            continue

        # 2. X√¢y d·ª±ng t√™n c·∫°nh ng∆∞·ª£c d·ª± ki·∫øn
        rev_rel = f"rev_{rel}"
        rev_edge_type = (dst, rev_rel, src)

        # 3. Ki·ªÉm tra xem c·∫°nh ng∆∞·ª£c n√†y c√≥ t·ªìn t·∫°i trong data kh√¥ng
        if rev_edge_type in data.edge_types:
            forward_edges.append(edge_type)
            reverse_edges.append(rev_edge_type)

    return forward_edges, reverse_edges


def prepare_data_splits(data, val_ratio=0.1, test_ratio=0.1):
    """
    S·ª≠ d·ª•ng RandomLinkSplit chu·∫©n c·ªßa PyG.
    """
    print("--- PREPARING DATA SPLITS (RandomLinkSplit) ---")

    # 1. T·ª± ƒë·ªông b·∫Øt c·∫∑p c·∫°nh ƒë·ªÉ x·ª≠ l√Ω Leakage
    target_edge_types, rev_edge_types = get_edge_pairs(data)
    for edge in target_edge_types:
        print(edge)
    print(f"-> Target Edges (Predicting): {len(target_edge_types)} types")

    # 2. C·∫•u h√¨nh Splitter
    transform = T.RandomLinkSplit(
        num_val=val_ratio,
        num_test=test_ratio,
        is_undirected=False,  # Hetero Graph c√≥ h∆∞·ªõng

        # [QUAN TR·ªåNG] Khai b√°o c·∫∑p c·∫°nh ƒë·ªÉ PyG t·ª± x√≥a c·∫°nh ng∆∞·ª£c trong message passing
        edge_types=target_edge_types,
        rev_edge_types=rev_edge_types,

        # T√°ch 30% c·∫°nh train ra l√†m "Label" (Supervision), 70% gi·ªØ l·∫°i n·ªëi d√¢y
        disjoint_train_ratio=0.3,

        add_negative_train_samples=False  # ƒê·ªÉ Loader t·ª± sinh m·∫´u √¢m -> Ti·∫øt ki·ªám RAM
    )

    # 3. Th·ª±c hi·ªán chia (T·∫°o ra 3 object Data ri√™ng bi·ªát)
    train_data, val_data, test_data = transform(data)

    return train_data, val_data, test_data, target_edge_types


def call_back(model):
    repo = ModelRepository(MODEL_PATH)
    repo.save_model(model)


def train_one_config(split_data, config, device, final_mode=False, trial=None):
    print("Initializing Data for Training!", flush=True, end=" ")
    train_data, val_data, test_data, target_edge_types = split_data
    print("Completed!", flush=True)
    hidden_dim = config['hidden_dim']
    lr = config['lr']
    epochs = config['epochs']
    batch_size = config['batch_size']
    dropout = config['dropout']
    print("Initializing Model!", flush=True, end=" ")
    model = LinkPredictionModel(hidden_dim, OUTPUT_DIM, train_data, dropout).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    print("Completed!", flush=True)

    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer,
        T_0=5,      # Reset LR m·ªói 5 epoch
        T_mult=2,   # L·∫ßn sau d√†i g·∫•p ƒë√¥i (5 -> 10 -> 20)
        eta_min=1e-5 # LR th·∫•p nh·∫•t
    )

    early_stop_patience = 10
    early_stop_counter = 0
    history = {"epoch": [], "loss": [], "val_auc": []}
    best_val_auc = 0.0
    final_test_auc = 0.0
    best_model_state = None
    best_epoch_found = 0

    print(f"\nHidden={hidden_dim}, LR={lr}, epochs={epochs}, dropout={dropout}, batch_size={batch_size}")
    scaler = torch.amp.GradScaler('cuda')  # 4. TRAINING LOOP
    for epoch in range(1, epochs + 1):
        loss = train_epoch(model, train_data, optimizer, device, target_edge_types, scaler, batch_size)

        history["epoch"].append(epoch)
        history["loss"].append(float(loss))
        log_msg = f"Epoch {epoch:03d} | Loss: {loss:.4f}"

        # ƒê√°nh gi√° tr√™n t·∫≠p Val ƒë·ªÉ ch·ªçn Model t·ªët nh·∫•t
        if not final_mode:
            val_auc = evaluate(model, val_data, device, target_edge_types, batch_size)
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            history["val_auc"].append(float(val_auc))
            log_msg += f" | Val AUC: {val_auc:.4f}| LR: {current_lr :.5f}"
            if val_auc > best_val_auc:
                best_val_auc = val_auc
                best_model_state = model.state_dict()  # L∆∞u l·∫°i state t·ªët nh·∫•t
                early_stop_counter = 0
                call_back(model)
            else:
                if val_auc == best_val_auc:
                    early_stop_counter += 1
                if early_stop_counter >= early_stop_patience:
                    print(
                        f"Early Stopping t·∫°i Epoch {epoch} v√¨ Val AUC kh√¥ng giao ƒë·ªông trong {early_stop_patience} epochs.")
                    break
                # 1. B√°o c√°o k·∫øt qu·∫£ hi·ªán t·∫°i cho Optuna
        print(log_msg)

    # 5. ƒê√ÅNH GI√Å CU·ªêI C√ôNG TR√äN T·∫¨P TEST
    if not final_mode and best_model_state:
        # Load l·∫°i tr·ªçng s·ªë t·ªët nh·∫•t (ƒë·∫°t ƒë·ªânh ·ªü Val) ƒë·ªÉ test
        model.load_state_dict(best_model_state)
        final_test_auc = evaluate(model, test_data, device, target_edge_types, batch_size)
        history['test_auc'] = final_test_auc
        print(f"--> Test AUC: {final_test_auc:.4f}")

    with open(TRAINING_HISTORY_PATH, "w", encoding="utf-8") as f:
        json.dump(history, f, indent=4)
    # N·∫øu l√† final mode th√¨ tr·∫£ v·ªÅ 1.0 (ho·∫∑c train auc)
    best_model = model
    final_test_auc = 1.0
    best_epoch_found = epochs

    # Tr·∫£ v·ªÅ Test AUC thay v√¨ Val AUC ƒë·ªÉ Grid Search in ra k·∫øt qu·∫£ th·ª±c t·∫ø h∆°n
    # Ho·∫∑c b·∫°n v·∫´n c√≥ th·ªÉ tr·∫£ v·ªÅ Val AUC ƒë·ªÉ ch·ªçn tham s·ªë, nh∆∞ng in Test AUC ƒë·ªÉ tham kh·∫£o

    torch.cuda.empty_cache()  # X·∫£ VRAM
    gc.collect()
    return best_val_auc, final_test_auc, best_model


# Garbage Collection ƒë·ªÉ d·ªçn RAM th·ªß c√¥ng
from optuna.pruners import MedianPruner
import torch

def is_edge_index_sorted(edge_index):
    """
    Ki·ªÉm tra xem edge_index c√≥ ƒë∆∞·ª£c s·∫Øp x·∫øp theo th·ª© t·ª± t·ª´ ƒëi·ªÉn
    (theo h√†ng source tr∆∞·ªõc, sau ƒë√≥ ƒë·∫øn h√†ng target) hay kh√¥ng.

    Args:
        edge_index (torch.Tensor): Tensor c√≥ k√≠ch th∆∞·ªõc [2, E], ki·ªÉu d·ªØ li·ªáu long/int.
                                   H√†ng 0 ch·ª©a source nodes, H√†ng 1 ch·ª©a target nodes.

    Returns:
        bool: True n·∫øu ƒë√£ s·∫Øp x·∫øp, False n·∫øu ch∆∞a.
    """

    # B∆∞·ªõc 1: Ki·ªÉm tra k√≠ch th∆∞·ªõc ƒë·∫ßu v√†o ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh h·ª£p l·ªá
    if edge_index.shape[0] != 2:
        raise ValueError(f"edge_index ph·∫£i c√≥ k√≠ch th∆∞·ªõc [2, E], hi·ªán t·∫°i l√† {edge_index.shape}")

    num_edges = edge_index.shape[1]

    # Tr∆∞·ªùng h·ª£p c∆° s·ªü: N·∫øu c√≥ 0 ho·∫∑c 1 c·∫°nh, m·∫∑c ƒë·ªãnh l√† ƒë√£ s·∫Øp x·∫øp
    if num_edges <= 1:
        return True

    # B∆∞·ªõc 2: T√°ch node ngu·ªìn (row 0) v√† node ƒë√≠ch (row 1)
    src = edge_index[0, :]
    dst = edge_index[1, :]

    # B∆∞·ªõc 3: T√≠nh s·ª± ch√™nh l·ªách (diff) gi·ªØa c√°c ph·∫ßn t·ª≠ li√™n ti·∫øp c·ªßa source
    # diff_src[i] = src[i+1] - src[i]
    diff_src = src[1:] - src[:-1]

    # ƒêi·ªÅu ki·ªán A: Source ph·∫£i kh√¥ng gi·∫£m (non-decreasing)
    # T·ª©c l√† src[i+1] >= src[i] => diff_src >= 0
    if torch.any(diff_src < 0):
        return False

    # B∆∞·ªõc 4: Ki·ªÉm tra ƒëi·ªÅu ki·ªán ph·ª• t·∫°i c√°c v·ªã tr√≠ m√† source b·∫±ng nhau
    # T√¨m c√°c ch·ªâ s·ªë (indices) m√† t·∫°i ƒë√≥ src[i+1] == src[i]
    # mask l√† tensor boolean: True t·∫°i nh·ªØng n∆°i source kh√¥ng ƒë·ªïi
    mask = (diff_src == 0)

    # N·∫øu kh√¥ng c√≥ ch·ªó n√†o source b·∫±ng nhau, v√† source ƒë√£ tƒÉng d·∫ßn (ƒë√£ check ·ªü tr√™n),
    # th√¨ coi nh∆∞ ƒë√£ s·∫Øp x·∫øp xong.
    if not torch.any(mask):
        return True

    # L·∫•y ra c√°c ph·∫ßn t·ª≠ c·ªßa ƒë√≠ch (dst) t∆∞∆°ng ·ª©ng v·ªõi v·ªã tr√≠ mask
    # dst[1:][mask] l√† dst[i+1] t·∫°i n∆°i src[i] == src[i+1]
    # dst[:-1][mask] l√† dst[i] t·∫°i n∆°i src[i] == src[i+1]

    relevant_dst_next = dst[1:][mask]
    relevant_dst_curr = dst[:-1][mask]

    # ƒêi·ªÅu ki·ªán B: T·∫°i nh·ªØng n∆°i source b·∫±ng nhau, target ph·∫£i kh√¥ng gi·∫£m
    # dst[i+1] >= dst[i]
    if torch.any(relevant_dst_next < relevant_dst_curr):
        return False

    return True

# --- PH·∫¶N SCRIPT TEST (TEST BENCH) ---

def run_test(data):
    l = torch.tensor([is_edge_index_sorted(data[edge_type].edge_index) for edge_type in data.edge_types])
    print(l.all())


def pre_process_data(data):
    # Gi·∫£ s·ª≠ data l√† HeteroData
    data.pin_memory()  # TƒÉng t·ªëc transfer d·ªØ li·ªáu
    return data


def run_optimization(data = None):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Running on: {device}")

    # --- B∆Ø·ªöC 1: CHU·∫®N B·ªä DATA CHO GRID SEARCH ---
    print("\n>>> Loading & Splitting Data...")

    # 1. Load d·ªØ li·ªáu g·ªëc
    #if data is None:
    data = get_or_prepare_data()


    run_test(data)
    # 2. C·∫Øt d·ªØ li·ªáu (In-Place Modification)
    # H√†m n√†y s·∫Ω x√≥a c·∫°nh Val/Test kh·ªèi `data` v√† tr·∫£ v·ªÅ indices r·ªùi
    # train_graph ch√≠nh l√† bi·∫øn `data` sau khi b·ªã c·∫Øt
    split_data = prepare_data_splits(data, val_ratio=0.005, test_ratio=0.005)
    config = {
        'hidden_dim': 128,
        'batch_size': 128,
        'lr': 0.01,
        'dropout': 0.41372892081262375,
        'epochs': 50
    }
    best_val_auc, test_auc, best_model = train_one_config(split_data, config, device)

    repo = ModelRepository(MODEL_PATH)
    repo.save_model(best_model)


# Loading data
#data = ToUndirected(merge=False)(data)
#feature.save_data(data,mapping)
# Training
if __name__ == "__main__":
    run_optimization()
>>>>>>> 9de2b1b (FINAL)
