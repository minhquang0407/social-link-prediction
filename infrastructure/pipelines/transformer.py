import sys
import json
import networkx as nx
import pandas as pd
import glob
import os
from unidecode import unidecode
from collections import defaultdict
from pathlib import Path
import pyarrow
from config.settings import RAW_JSON_DIR, RAW_PARQUET_PATH, CLEAN_DATA_PATH, GRAPH_PATH
from infrastructure.repositories import PickleGraphRepository
class GraphTransformer:
    def __init__(self):
        # Kh·ªüi t·∫°o m·ªôt ƒë·ªì th·ªã r·ªóng
        print("GraphTransformer initialized.")

    def _ingest_json_to_parquet(self,json_folder):
        json_folder_path = Path(json_folder)
        json_files = glob.glob(os.path.join(str(json_folder_path), "*.json"))

        dfs = []
        for file_path in json_files:
            file_name = os.path.basename(file_path)
            try:
                file_name = os.path.splitext(file_name)[0].split('_')

                object_type = file_name[-1]
            except IndexError:
                object_type = "unknown"
            try:
                print(f"Chuy·ªÉn ƒë·ªïi quan h·ªá {file_name[-2]}:",end='',flush=True)
                # ƒê·ªçc d·ªØ li·ªáu
                data = self._load_and_flatten_json(file_path)

                # N·∫øu file r·ªóng ho·∫∑c l·ªói, b·ªè qua
                if data.empty: continue

                data['objectType.value'] = object_type
                dfs.append(data)

                print("Th√†nh c√¥ng!",flush=True)
            except Exception as e:
                print(f"L·ªñI: {e}")


        # G·ªôp t·∫•t c·∫£ d·ªØ li·ªáu ch√≠nh
        if dfs:
            dfs_final = pd.concat(dfs, ignore_index=True)
        else:
            print("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ch√≠nh!")
            return

        interest_path = json_folder_path / "interest"
        interest_files = glob.glob(os.path.join(str(interest_path), "*interest*.json"))

        dfs_interests = []
        for file_path in interest_files:
            # ƒê·ªçc d·ªØ li·ªáu
            data = self._load_and_flatten_json(file_path)
            if data.empty: continue

            # 1. S·ª¨A L·ªñI CH·ªåN C·ªòT: Ch·ªçn ƒë√∫ng c·ªôt c·∫ßn thi·∫øt
            if 'person.value' in data.columns and 'objectLabel.value' in data.columns:
                data = data[['person.value', 'objectLabel.value']]
                data = data.rename(columns={'objectLabel.value': 'interests.value'})
                dfs_interests.append(data)

        # --- PH·∫¶N 3: MERGE (G·ªòP S·ªû TH√çCH V√ÄO MAIN) ---
        if dfs_interests:
            # G·ªôp t·∫•t c·∫£ file interest l·∫°i
            df_interests_all = pd.concat(dfs_interests, ignore_index=True)

            df_interests_agg = df_interests_all.groupby('person.value')['interests.value'].apply(
                lambda x: ', '.join(x.dropna().astype(str).unique())
            ).reset_index()

            df_final = pd.merge(dfs_final, df_interests_agg, on='person.value', how='left')
        else:
            print("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu Interest, b·ªè qua b∆∞·ªõc merge.")
            df_final = dfs_final

        # --- PH·∫¶N 4: L∆ØU PARQUET ---
        os.makedirs(os.path.dirname(str(RAW_PARQUET_PATH)), exist_ok=True)

        df_final.to_parquet(str(RAW_PARQUET_PATH), engine='pyarrow', compression='snappy')
        print(f"ƒê√£ g·ªôp xong! T·ªïng s·ªë d√≤ng: {len(df_final)}")
        return df_final

    def _load_and_flatten_json(self, raw_filepath):
        """
        ƒê·ªçc, l√†m ph·∫≥ng v√† d·ªçn d·∫πp s∆° b·ªô d·ªØ li·ªáu t·ª´ JSON.
        """
        if not os.path.exists(raw_filepath):
            print(f"‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y file {raw_filepath}")
            return pd.DataFrame()

        try:
            with open(raw_filepath, "r", encoding="utf-8") as f:
                data = json.load(f)

            # 1. L√†m ph·∫≥ng
            bindings = data.get('results', {}).get('bindings', [])
            if not bindings:
                return pd.DataFrame()

            df = pd.json_normalize(bindings)


            return df
        except Exception as e:
            print(f"‚ùå L·ªói khi ƒë·ªçc file {raw_filepath}: {e}")
            return pd.DataFrame()

    def _clean_and_procces_data(self, df: pd.DataFrame) -> pd.DataFrame:
        if df.empty:
            return df
        # 2. ƒê·ªïi t√™n c·ªôt (B·ªè ƒëu√¥i .value)
        # Ch·ªâ ƒë·ªïi t√™n nh·ªØng c·ªôt quan tr·ªçng, c√°c c·ªôt kh√°c (.type, .xml:lang) s·∫Ω b·ªã l·ªçc b·ªè sau
        new_columns = {col: col.replace('.value', '') \
                       for col in df.columns \
                       if col.endswith('.value')}

        #ƒê·ªïi t√™n c·ªôt
        df = df.rename(columns=new_columns)

        # 3. L·ªçc b·ªè c√°c c·ªôt metadata th·ª´a (type, xml:lang, datatype...)

        valid_cols = list(new_columns.values())
        df = df[valid_cols]

        num_col_isnull = df.isnull().sum()
        print(f"Th·ªëng k√™ c·ªôt c√≥ d·ªØ li·ªáu b·ªã thi·∫øu:\n{num_col_isnull}",flush=True)
        num_row_isnull = df.isnull().any(axis=1).sum()
        print(f"T·ªïng s·ªë d√≤ng c√≥ d·ªØ li·ªáu b·ªã thi·∫øu: {num_row_isnull}",flush=True)
        for col in df.columns:
            df[col] = df[col].fillna("").astype(str).str.strip().str.replace(r'[\r\n\t]+', ' ', regex=True)


        # L√†m s·∫°ch ID (b·ªè http://.../Q123 -> Q123)
        for col in ['person', 'object']:
            if col in df.columns:
                df[col] = df[col].astype(str).str.split('/').str[-1]

        # L·ªçc c√°c d√≤ng m√† c√≥ name b·∫Øt ƒë·∫ßu b·∫±ng Q...
        qid_pattern = r'^Q\d+$'
        total_dropped = 0

        if 'personLabel' in df.columns:
            mask_invalid = df['personLabel'].astype(str).str.match(qid_pattern, na=False)
            count_invalid = mask_invalid.sum()
            if count_invalid > 0:
                df = df[~mask_invalid]
                total_dropped += count_invalid

        if 'objectLabel' in df.columns:
            mask_invalid = df['objectLabel'].astype(str).str.match(qid_pattern, na=False)
            count_invalid = mask_invalid.sum()
            if count_invalid > 0:
                df = df[~mask_invalid]
                total_dropped += count_invalid

        print(f"S·ªë d√≤ng b·ªã b·ªè (Name l·ªói): {total_dropped}", flush=True)

        # L·ªçc d√≤ng tr·ªëng ID
        df = df[df['person'].notna() & (df['person'] != '')]


        # L∆∞u file


        print("ƒê√£ l√†m s·∫°ch xong!")
        return df

    def _create_attribute_node(self, df: pd.DataFrame) -> dict:
        # -- X·ª¨ L√ù PERSON --
        cols_person = {
            'person': 'id',
            'personLabel': 'name',
            'personDescription': 'description',
            'birthYear': 'birthYear',
            'interest': 'interests',
            'countryLabel': 'country',
            'birthPlaceLabel': 'birthPlace'
        }

        valid_p_cols = [c for c in cols_person.keys() if c in df.columns]
        df_p = df[valid_p_cols].drop_duplicates(subset=['person'])
        df_p['type'] = 'human'
        df_p['normalize_name'] = df_p['personLabel'].astype(str).apply(unidecode).str.lower()
        df_p.rename(columns=cols_person, inplace=True)

        # -- X·ª¨ L√ù OBJECT --
        cols_object = {
            'object': 'id',
            'objectLabel': 'name',
            'objectDescription': 'description',
            'objectType': 'type'
        }
        valid_o_cols = [c for c in cols_object.keys() if c in df.columns]
        df_o = df[valid_o_cols].drop_duplicates(subset=['object']).copy()
        df_o['normalize_name'] = df_o['objectLabel'].astype(str).apply(unidecode).str.lower()
        df_o.rename(columns=cols_object, inplace=True)

        # -- V√å PERSON V√Ä OBJECT ƒê·ªÄU L√Ä NODE N√äN G·ªòP L·∫†I ƒê·ªÇ TH√äM 1 L·∫¶N
        df_all = pd.concat([df_p, df_o], ignore_index=True)
        df_all = df_all.drop_duplicates(subset=['id'], keep= 'first')
        df_all = df_all.set_index('id')

        # Chuy·ªÉn DF th√†nh Dict of Dicts
        # orient='index' t·∫°o ra: {ID_Node: {attr1: val1, attr2: val2}}
        node_attrs = df_all.to_dict(orient='index')
        return node_attrs

    def build_graph(self, df):
        # 1. T·∫°o ƒë·ªì th·ªã c∆° b·∫£n
        G = nx.from_pandas_edgelist(
            df,
            source='person',
            target='object',
            edge_attr= 'relationshipLabel',
            create_using=nx.DiGraph
        )
        print(f"Graph Stat: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.")


        # --- C·∫¨P NH·∫¨T V√ÄO NETWORKX ---
        node_attrs = self._create_attribute_node(df)
        nx.set_node_attributes(G, node_attrs)
        print("ƒê√£ c·∫≠p nh·∫≠t thu·ªôc t√≠nh Node th√†nh c√¥ng.")
        return G

    def run_transformer(self, raw_dir = None, force_data = True ):
        """
        H√†m ƒëi·ªÅu ph·ªëi ch√≠nh (Orchestrator).
        """

        # --- B∆Ø·ªöC 1: L·∫§Y D·ªÆ LI·ªÜU C·∫†NH (EDGES) ---

        print(f"üöÄ Ch·∫°y pipeline t·ª´ Raw Directory: {raw_dir}")
        if force_data:
            df = self._ingest_json_to_parquet(raw_dir)
            df = self._clean_and_procces_data(df)
        else:
            df = self._ingest_json_to_parquet(CLEAN_DATA_PATH)

        os.makedirs(os.path.dirname(str(CLEAN_DATA_PATH)), exist_ok=True)
        df.to_parquet(str(CLEAN_DATA_PATH), engine='pyarrow', compression='snappy')

        relationship_graph = self.build_graph(df)

        return relationship_graph




