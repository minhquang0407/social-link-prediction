{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import sort_edge_index\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "import itertools\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from infrastructure.repositories import PyGDataRepository\n",
    "from config.settings import MAPPING_PATH, PYG_DATA_PATH, OUTPUT_DIM, TRAINING_HISTORY_PATH, MODEL_PATH, PYG_TRAINING_DATA_PATH\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Architecture"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Function to train"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v5uNPL-8ToWD"
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import sort_edge_index\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "import itertools\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from infrastructure.repositories import PyGDataRepository\n",
    "from config.settings import  PYG_DATA_PATH, OUTPUT_DIM, TRAINING_HISTORY_PATH, MODEL_PATH, \\\n",
    "    PYG_TRAINING_DATA_PATH\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "\n",
    "# Architecture\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self,hidden_channels, out_channels, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1,-1), hidden_channels)\n",
    "        self.ln1 = torch.nn.LayerNorm(hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1,-1), out_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.ln1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.normalize(x, p=2., dim=-1)\n",
    "        return x\n",
    "\n",
    "class InteractionMLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        # Input dim * 3 v√¨ ta s·∫Ω n·ªëi [src, dst, src*dst]\n",
    "        self.lin1 = torch.nn.Linear(input_dim * 3, hidden_dim)\n",
    "        self.lin2 = torch.nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.lin3 = torch.nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, z_src, z_dst):\n",
    "        # T·∫†O T∆Ø∆†NG T√ÅC M·∫†NH M·∫º\n",
    "        # 1. ƒê·∫∑c tr∆∞ng g·ªëc: z_src, z_dst\n",
    "        # 2. ƒê·∫∑c tr∆∞ng t∆∞∆°ng ƒë·ªìng: z_src * z_dst (Hadamard product)\n",
    "        # Gi√∫p model d·ªÖ d√†ng h·ªçc ƒë∆∞·ª£c \"A v√† B c√≥ gi·ªëng nhau kh√¥ng?\"\n",
    "        combined = torch.cat([z_src, z_dst, z_src * z_dst], dim=1)\n",
    "\n",
    "        h = self.lin1(combined)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        h = self.lin2(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        h = self.lin3(h)\n",
    "\n",
    "        # √âp v·ªÅ 1 chi·ªÅu ƒë·ªÉ tr√°nh l·ªói shape [N, 1] vs [N]\n",
    "        return h.view(-1)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HGTConv, Linear\n",
    "\n",
    "class HGTLinkPrediction(torch.nn.Module):\n",
    "    def __init__(self,  hidden_channels, out_channels,data, dropout=0.5, num_heads=4, num_layers=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. INPUT PROJECTION (Quan tr·ªçng)\n",
    "        # Bi·∫øn ƒë·ªïi vector Text (768 dim) + Year (1 dim) v·ªÅ kh√¥ng gian chung (256 dim)\n",
    "        # Gi√∫p model h·ªçc ƒë∆∞·ª£c ƒë·∫∑c tr∆∞ng ri√™ng cho b√†i to√°n n√†y\n",
    "        self.lin_dict = torch.nn.ModuleDict()\n",
    "        for node_type in data.node_types:\n",
    "            # L·∫•y k√≠ch th∆∞·ªõc feature ƒë·∫ßu v√†o th·ª±c t·∫ø t·ª´ data\n",
    "            in_dim = data[node_type].x.size(1)\n",
    "            self.lin_dict[node_type] = Linear(in_dim, hidden_channels)\n",
    "\n",
    "        # 2. HGT LAYERS (Thay cho SAGE)\n",
    "        # HGT d√πng c∆° ch·∫ø Attention ƒë·ªÉ \"ƒë·ªçc hi·ªÉu\" feature text t·ªët h∆°n SAGE\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HGTConv(hidden_channels, hidden_channels, data.metadata(),\n",
    "                           heads=num_heads)\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        # 3. INTERACTION DECODER (Nh∆∞ ƒë√£ b√†n)\n",
    "        self.decoders = torch.nn.ModuleDict()\n",
    "        unique_rels = set()\n",
    "        for src, rel, dst in data.edge_types:\n",
    "            if not rel.startswith('rev_'): # B·ªè qua c·∫°nh ng∆∞·ª£c\n",
    "                unique_rels.add(rel)\n",
    "\n",
    "        for rel in unique_rels:\n",
    "            key = f\"__{rel}__\"\n",
    "            self.decoders[key] = InteractionMLP(out_channels, 64, 1, dropout)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, target_edge_type, edge_label_index):\n",
    "        # A. Projection: √âp feature text + year v√†o kh√¥ng gian Hidden\n",
    "        dtype = self.kqv_lin.weights[0].dtype if hasattr(self.kqv_lin, 'weights') else torch.float32\n",
    "        x_dict = {k: v.to(dtype) for k, v in x_dict.items()}\n",
    "        x_start = {}\n",
    "        for node_type, x in x_dict.items():\n",
    "            x_start[node_type] = self.lin_dict[node_type](x).relu_()\n",
    "            x_start[node_type] = self.dropout(x_start[node_type])\n",
    "\n",
    "        # B. HGT Message Passing\n",
    "        for conv in self.convs:\n",
    "            x_start = conv(x_start, edge_index_dict)\n",
    "\n",
    "        # C. Decode\n",
    "        src_type, rel, dst_type = target_edge_type\n",
    "        z_src = x_start[src_type][edge_label_index[0]]\n",
    "        z_dst = x_start[dst_type][edge_label_index[1]]\n",
    "\n",
    "        key = f\"__{rel}__\"\n",
    "        if key in self.decoders:\n",
    "            return self.decoders[key](z_src, z_dst)\n",
    "        else:\n",
    "            return torch.zeros(z_src.size(0), device=z_src.device)\n",
    "\n",
    "class LinkPredictionModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, data=None, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        # D√πng DeepGraphSAGE thay v√¨ b·∫£n th∆∞·ªùng\n",
    "        self.gnn = GraphSAGE(hidden_channels, out_channels, dropout)\n",
    "        self.encoder = to_hetero(self.gnn, data.metadata(), aggr='sum')\n",
    "\n",
    "        self.decoders = torch.nn.ModuleDict()\n",
    "\n",
    "        # Init Decoder cho t·ª´ng lo·∫°i c·∫°nh\n",
    "        unique_rels = set()\n",
    "        for src, rel, dst in data.edge_types:\n",
    "            if not rel.startswith('rev_'): # B·ªè qua c·∫°nh ng∆∞·ª£c\n",
    "                unique_rels.add(rel)\n",
    "\n",
    "        for rel in unique_rels:\n",
    "            key = f\"__{rel}__\"\n",
    "            self.decoders[key] = InteractionMLP(out_channels, 64, 1, dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, target_edge_type, edge_label_index):\n",
    "        z_dict = self.encoder(x, edge_index)\n",
    "\n",
    "        src_type, rel, dst_type = target_edge_type\n",
    "\n",
    "        z_src = z_dict[src_type][edge_label_index[0]]\n",
    "        z_dst = z_dict[dst_type][edge_label_index[1]]\n",
    "\n",
    "        key = f\"__{rel}__\"\n",
    "\n",
    "        if key in self.decoders:\n",
    "            return self.decoders[key](z_src, z_dst)\n",
    "        else:\n",
    "            return torch.zeros(z_src.size(0), device=z_src.device)\n",
    "\n",
    "# Function to train\n",
    "import optuna\n",
    "\n",
    "from config.settings import BATCH_SIZE, MODEL_PATH\n",
    "from infrastructure.repositories import ModelRepository\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "def get_or_prepare_data():\n",
    "    \"\"\"T·∫£i v√† chu·∫©n b·ªã d·ªØ li·ªáu (Undirected + Sanitize).\"\"\"\n",
    "    feature_repo = PyGDataRepository(PYG_DATA_PATH)\n",
    "    data = feature_repo.load_data()\n",
    "\n",
    "    if data is None:\n",
    "        print(\"Ch∆∞a c√≥ d·ªØ li·ªáu PyG. Vui l√≤ng ch·∫°y ETL tr∆∞·ªõc!\")\n",
    "        return None\n",
    "\n",
    "    # X√≥a c·∫°nh r·ªóng\n",
    "    #data = sanitize_hetero_data(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def loader_generator(data_source, target_edge_types, batch_size, shuffle=False):\n",
    "    for et in target_edge_types:\n",
    "        # 1. Ki·ªÉm tra nhanh (gi·ªØ nguy√™n logic c≈© c·ªßa b·∫°n)\n",
    "        if et not in data_source.edge_index_dict: pass\n",
    "        if hasattr(data_source[et], 'edge_label_index') and data_source[et].edge_label_index.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        # 2. Chu·∫©n b·ªã nh√£n\n",
    "        lbl_index = data_source[et].edge_label_index\n",
    "        lbl_ones = torch.ones(lbl_index.size(1), dtype=torch.float32)\n",
    "\n",
    "        # 3. Kh·ªüi t·∫°o Loader (Ch·ªâ t·ªën RAM t·∫°i th·ªùi ƒëi·ªÉm n√†y)\n",
    "\n",
    "        loader = LinkNeighborLoader(\n",
    "            data_source,\n",
    "            num_neighbors=[50, 30],\n",
    "            edge_label_index=(et, lbl_index),\n",
    "            edge_label=lbl_ones,\n",
    "            neg_sampling_ratio=3.0,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=0\n",
    "        )\n",
    "        # 4. Tr·∫£ v·ªÅ ƒë·ªÉ d√πng ngay\n",
    "        yield et, loader\n",
    "\n",
    "\n",
    "def train_epoch(model, data, optimizer, device, target_edge_types, scaler, batch_size=BATCH_SIZE):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_examples = 0\n",
    "    count = 1\n",
    "    max = len(target_edge_types)\n",
    "    data_loader_gen = loader_generator(data, target_edge_types, batch_size, shuffle=True)\n",
    "    for edge_type, loader in data_loader_gen:\n",
    "        pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "        pbar.set_postfix({\n",
    "            \"relationship\": f\" {edge_type[1]} | {count}/{max}\"\n",
    "        })\n",
    "        for batch in pbar:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # V·ªõi single-type loader, ta bi·∫øt ch·∫Øc ch·∫Øn c·∫°nh c·∫ßn d·ª± ƒëo√°n l√† edge_type\n",
    "            # PyG t·ª± ƒë·ªông g√°n v√†o edge_label_index c·ªßa edge_type ƒë√≥ trong batch\n",
    "            edge_label_index = batch[edge_type].edge_label_index\n",
    "            edge_label = batch[edge_type].edge_label\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                # Forward\n",
    "                out = model(batch.x_dict, batch.edge_index_dict, edge_type, edge_label_index)\n",
    "\n",
    "                # Loss\n",
    "                loss = F.binary_cross_entropy_with_logits(out, edge_label)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # 3. Optimizer Step\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()  # C·∫≠p nh·∫≠t l·∫°i h·ªá s·ªë scale cho l·∫ßn sau\n",
    "\n",
    "            total_loss += loss.item() * edge_label.size(0)\n",
    "            total_examples += edge_label.size(0)\n",
    "        count += 1\n",
    "        del loader, pbar\n",
    "        gc.collect()\n",
    "    return total_loss / (total_examples + 1e-6)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, device, target_edge_types, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p Val ho·∫∑c Test.\n",
    "\n",
    "    Args:\n",
    "        model: M√¥ h√¨nh GNN ƒë√£ hu·∫•n luy·ªán.\n",
    "        loaders: List c√°c LinkNeighborLoader (m·ªói loader ·ª©ng v·ªõi 1 lo·∫°i c·∫°nh).\n",
    "        device: 'cuda' ho·∫∑c 'cpu'.\n",
    "        target_edge_types: List c√°c lo·∫°i c·∫°nh t∆∞∆°ng ·ª©ng v·ªõi loaders.\n",
    "\n",
    "    Returns:\n",
    "        score: Ch·ªâ s·ªë ROC-AUC (0.0 -> 1.0).\n",
    "    \"\"\"\n",
    "    model.eval()  # Chuy·ªÉn model sang ch·∫ø ƒë·ªô ƒë√°nh gi√° (t·∫Øt Dropout, kh√≥a BatchNorm)\n",
    "\n",
    "    preds = []\n",
    "    ground_truths = []\n",
    "\n",
    "    # 1. Duy·ªát song song qua t·ª´ng c·∫∑p (Lo·∫°i c·∫°nh, Loader t∆∞∆°ng ·ª©ng)\n",
    "    # L∆∞u √Ω: target_edge_types v√† loaders ph·∫£i c√≥ c√πng ƒë·ªô d√†i v√† th·ª© t·ª±\n",
    "    count = 1\n",
    "    max = len(target_edge_types)\n",
    "    data_loader_gen = loader_generator(data, target_edge_types, batch_size, shuffle=False)\n",
    "    for edge_type, loader in data_loader_gen:\n",
    "\n",
    "        pbar = tqdm(loader, desc=\"Validation\", leave=False)\n",
    "        pbar.set_postfix({\n",
    "            \"relationship\": f\" {edge_type[1]} | {count}/{max}\"\n",
    "        })\n",
    "        for batch in pbar:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Ki·ªÉm tra an to√†n: Batch c√≥ ch·ª©a nh√£n cho lo·∫°i c·∫°nh n√†y kh√¥ng?\n",
    "            if not hasattr(batch[edge_type], 'edge_label_index') or batch[edge_type].edge_label_index.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            # L·∫•y d·ªØ li·ªáu \"ƒë·ªÅ thi\"\n",
    "            edge_label_index = batch[edge_type].edge_label_index\n",
    "            edge_label = batch[edge_type].edge_label\n",
    "\n",
    "            # 3. Forward Pass\n",
    "            # Truy·ªÅn ƒë√∫ng edge_type ƒë·ªÉ model bi·∫øt d√πng tr·ªçng s·ªë n√†o (n·∫øu c√≥ chia t√°ch)\n",
    "            # Output model th∆∞·ªùng l√† Logits (ch∆∞a qua Sigmoid)\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                out = model(batch.x_dict, batch.edge_index_dict, edge_type, edge_label_index)\n",
    "                # Sigmoid c≈©ng n√™n n·∫±m trong context n√†y (ho·∫∑c kh√¥ng, t√πy √Ω, nh∆∞ng forward model b·∫Øt bu·ªôc ph·∫£i c√≥)\n",
    "                out = torch.sigmoid(out)\n",
    "\n",
    "            # 4. L∆∞u l·∫°i k·∫øt qu·∫£ (ƒê∆∞a v·ªÅ CPU v√† Numpy ƒë·ªÉ t√≠nh to√°n b·∫±ng Sklearn)\n",
    "            preds.append(out.cpu().numpy())\n",
    "            ground_truths.append(edge_label.cpu().numpy())\n",
    "        count += 1\n",
    "        del loader, pbar\n",
    "    # 5. X·ª≠ l√Ω tr∆∞·ªùng h·ª£p kh√¥ng c√≥ d·ªØ li·ªáu (tr√°nh l·ªói crash)\n",
    "    if len(preds) == 0:\n",
    "        print(\"Not data for validation.\")\n",
    "        return 0.0\n",
    "\n",
    "    # 6. G·ªôp t·∫•t c·∫£ c√°c m·∫£ng numpy l·∫°i th√†nh 1 m·∫£ng d√†i duy nh·∫•t\n",
    "    final_preds = np.concatenate(preds)\n",
    "    final_labels = np.concatenate(ground_truths)\n",
    "\n",
    "    if np.isnan(final_preds).any():\n",
    "        print(\"‚ùå L·ªñI NGHI√äM TR·ªåNG: Model output ch·ª©a NaN!\")\n",
    "        return 0.0\n",
    "\n",
    "    # Ki·ªÉm tra xem Labels c√≥ ƒë·ªß 2 l·ªõp (0 v√† 1) kh√¥ng\n",
    "    unique_labels = np.unique(final_labels)\n",
    "    if len(unique_labels) < 2:\n",
    "        print(f\"‚ö†Ô∏è C·∫¢NH B√ÅO: T·∫≠p Label ch·ªâ ch·ª©a 1 lo·∫°i nh√£n duy nh·∫•t: {unique_labels}\")\n",
    "        print(\"-> L√Ω do: Loader Validation ch∆∞a b·∫≠t 'neg_sampling_ratio'!\")\n",
    "        return 0.0\n",
    "\n",
    "    # 7. T√≠nh ROC-AUC Score\n",
    "    try:\n",
    "        return roc_auc_score(final_labels, final_preds)\n",
    "    except ValueError as e:\n",
    "        print(f\"‚ùå SKLEARN ERROR: {e}\")\n",
    "        # In th√™m th·ªëng k√™ ƒë·ªÉ bi·∫øt t·∫°i sao\n",
    "        print(f\"Min Pred: {final_preds.min()}, Max Pred: {final_preds.max()}\")\n",
    "        print(f\"Unique Labels: {np.unique(final_labels)}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# --- 3. CHI·∫æN L∆Ø·ª¢C CH·∫†Y ---\n",
    "def get_edge_pairs(data):\n",
    "    \"\"\"\n",
    "    T·ª± ƒë·ªông b·∫Øt c·∫∑p c·∫°nh thu·∫≠n v√† c·∫°nh ngh·ªãch.\n",
    "    Quy t·∫Øc: C·∫°nh ngh·ªãch c√≥ th√™m ti·ªÅn t·ªë 'rev_' ho·∫∑c l√† chi·ªÅu ng∆∞·ª£c l·∫°i.\n",
    "    \"\"\"\n",
    "    forward_edges = []\n",
    "    reverse_edges = []\n",
    "\n",
    "    for edge_type in data.edge_types:\n",
    "        src, rel, dst = edge_type\n",
    "\n",
    "        # 1. B·ªè qua n·∫øu ƒë√¢y l√† c·∫°nh 'rev_' (ch√∫ng ta s·∫Ω x·ª≠ l√Ω n√≥ khi g·∫∑p c·∫°nh thu·∫≠n)\n",
    "        if rel.startswith('rev_'):\n",
    "            continue\n",
    "        if src != 'human' or dst != 'human':\n",
    "            continue\n",
    "        # 2. X√¢y d·ª±ng t√™n c·∫°nh ng∆∞·ª£c d·ª± ki·∫øn\n",
    "        rev_rel = f\"rev_{rel}\"\n",
    "        rev_edge_type = (dst, rev_rel, src)\n",
    "\n",
    "        # 3. Ki·ªÉm tra xem c·∫°nh ng∆∞·ª£c n√†y c√≥ t·ªìn t·∫°i trong data kh√¥ng\n",
    "        if rev_edge_type in data.edge_types:\n",
    "            forward_edges.append(edge_type)\n",
    "            reverse_edges.append(rev_edge_type)\n",
    "\n",
    "    return forward_edges, reverse_edges\n",
    "\n",
    "\n",
    "def prepare_data_splits(data, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    S·ª≠ d·ª•ng RandomLinkSplit chu·∫©n c·ªßa PyG.\n",
    "    \"\"\"\n",
    "    print(\"--- PREPARING DATA SPLITS (RandomLinkSplit) ---\")\n",
    "\n",
    "    # 1. T·ª± ƒë·ªông b·∫Øt c·∫∑p c·∫°nh ƒë·ªÉ x·ª≠ l√Ω Leakage\n",
    "    target_edge_types, rev_edge_types = get_edge_pairs(data)\n",
    "    for edge in target_edge_types:\n",
    "        print(edge)\n",
    "    print(f\"-> Target Edges (Predicting): {len(target_edge_types)} types\")\n",
    "\n",
    "    # 2. C·∫•u h√¨nh Splitter\n",
    "    transform = T.RandomLinkSplit(\n",
    "        num_val=val_ratio,\n",
    "        num_test=test_ratio,\n",
    "        is_undirected=False,  # Hetero Graph c√≥ h∆∞·ªõng\n",
    "\n",
    "        edge_types=target_edge_types,\n",
    "        rev_edge_types=rev_edge_types,\n",
    "\n",
    "        # T√°ch 20% c·∫°nh train ra l√†m \"Label\" (Supervision), 80% gi·ªØ l·∫°i n·ªëi d√¢y\n",
    "        disjoint_train_ratio=0.2,\n",
    "\n",
    "        add_negative_train_samples=False\n",
    "    )\n",
    "\n",
    "    # 3. Th·ª±c hi·ªán chia (T·∫°o ra 3 object Data ri√™ng bi·ªát)\n",
    "    train_data, val_data, test_data = transform(data)\n",
    "\n",
    "    return train_data, val_data, test_data, target_edge_types\n",
    "\n",
    "\n",
    "def call_back(model):\n",
    "    repo = ModelRepository(MODEL_PATH)\n",
    "    repo.save_model(model)\n",
    "\n",
    "def update_training_history(file_path, new_data):\n",
    "    # B∆∞·ªõc 1: Kh·ªüi t·∫°o d·ªØ li·ªáu hi·ªán c√≥\n",
    "    history = []\n",
    "\n",
    "    # B∆∞·ªõc 2: Ki·ªÉm tra v√† ƒê·ªçc d·ªØ li·ªáu c≈©\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                history = json.load(f)\n",
    "                # ƒê·∫£m b·∫£o history l√† m·ªôt list ƒë·ªÉ c√≥ th·ªÉ append\n",
    "                if not isinstance(history, list):\n",
    "                    history = [history]\n",
    "        except (json.JSONDecodeError, ValueError):\n",
    "            # X·ª≠ l√Ω n·∫øu file tr·ªëng ho·∫∑c l·ªói ƒë·ªãnh d·∫°ng\n",
    "            history = []\n",
    "\n",
    "    # B∆∞·ªõc 3: C·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi\n",
    "    if isinstance(new_data, list):\n",
    "        history.extend(new_data)\n",
    "    else:\n",
    "        history.append(new_data)\n",
    "\n",
    "    # B∆∞·ªõc 4: Ghi ƒë√® l·∫°i to√†n b·ªô file v·ªõi mode \"w\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(history, f, indent=4, ensure_ascii=False)\n",
    "def train_one_config(split_data, config, device, final_mode=False, trial=None):\n",
    "    print(\"Initializing Data for Training!\", flush=True, end=\" \")\n",
    "    train_data, val_data, test_data, target_edge_types = split_data\n",
    "    print(\"Completed!\", flush=True)\n",
    "    hidden_dim = config['hidden_dim']\n",
    "    lr = config['lr']\n",
    "    epochs = config['epochs']\n",
    "    batch_size = config['batch_size']\n",
    "    dropout = config['dropout']\n",
    "    print(\"Initializing Model!\", flush=True, end=\" \")\n",
    "    model = LinkPredictionModel(hidden_dim, OUTPUT_DIM, train_data, dropout).to(device)\n",
    "    state_dict = torch.load(MODEL_PATH,weights_only= False, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    print(\"Completed!\", flush=True)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=5,      # Reset LR m·ªói 5 epoch\n",
    "        T_mult=2,   # L·∫ßn sau d√†i g·∫•p ƒë√¥i (5 -> 10 -> 20)\n",
    "        eta_min=1e-5 # LR th·∫•p nh·∫•t\n",
    "    )\n",
    "\n",
    "    early_stop_patience = 10\n",
    "    early_stop_counter = 0\n",
    "    history = {\"epoch\": [], \"loss\": [], \"val_auc\": []}\n",
    "    best_val_auc = 0.7485\n",
    "    final_test_auc = 0.0\n",
    "    best_model_state = None\n",
    "    best_epoch_found = 0\n",
    "\n",
    "    print(f\"\\nHidden={hidden_dim}, LR={lr}, epochs={epochs}, dropout={dropout}, batch_size={batch_size}\")\n",
    "    scaler = torch.amp.GradScaler('cuda')  # 4. TRAINING LOOP\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train_epoch(model, train_data, optimizer, device, target_edge_types, scaler, batch_size)\n",
    "\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"loss\"].append(float(loss))\n",
    "        log_msg = f\"Epoch {epoch:03d} | Loss: {loss:.4f}\"\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        # ƒê√°nh gi√° tr√™n t·∫≠p Val ƒë·ªÉ ch·ªçn Model t·ªët nh·∫•t\n",
    "        val_auc = evaluate(model, val_data, device, target_edge_types, batch_size)\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history[\"val_auc\"].append(float(val_auc))\n",
    "        log_msg += f\" | Val AUC: {val_auc:.4f}| LR: {current_lr :.5f}\"\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_model_state = model.state_dict()  # L∆∞u l·∫°i state t·ªët nh·∫•t\n",
    "            early_stop_counter = 0\n",
    "            call_back(model)\n",
    "        else:\n",
    "            if val_auc == best_val_auc:\n",
    "                early_stop_counter += 1\n",
    "            if early_stop_counter >= early_stop_patience:\n",
    "                print(\n",
    "                    f\"Early Stopping t·∫°i Epoch {epoch} v√¨ Val AUC kh√¥ng giao ƒë·ªông trong {early_stop_patience} epochs.\")\n",
    "                break\n",
    "        update_training_history(TRAINING_HISTORY_PATH,history)\n",
    "        print(log_msg)\n",
    "\n",
    "    # 5. ƒê√ÅNH GI√Å CU·ªêI C√ôNG TR√äN T·∫¨P TEST\n",
    "    if not final_mode and best_model_state:\n",
    "        # Load l·∫°i tr·ªçng s·ªë t·ªët nh·∫•t (ƒë·∫°t ƒë·ªânh ·ªü Val) ƒë·ªÉ test\n",
    "        model.load_state_dict(best_model_state)\n",
    "        final_test_auc = evaluate(model, test_data, device, target_edge_types, batch_size)\n",
    "        history['test_auc'] = final_test_auc\n",
    "        print(f\"--> Test AUC: {final_test_auc:.4f}\")\n",
    "\n",
    "    update_training_history(TRAINING_HISTORY_PATH,history)\n",
    "\n",
    "    # N·∫øu l√† final mode th√¨ tr·∫£ v·ªÅ 1.0 (ho·∫∑c train auc)\n",
    "    best_model = model\n",
    "    final_test_auc = 1.0\n",
    "    best_epoch_found = epochs\n",
    "\n",
    "    # Tr·∫£ v·ªÅ Test AUC thay v√¨ Val AUC ƒë·ªÉ Grid Search in ra k·∫øt qu·∫£ th·ª±c t·∫ø h∆°n\n",
    "    # Ho·∫∑c b·∫°n v·∫´n c√≥ th·ªÉ tr·∫£ v·ªÅ Val AUC ƒë·ªÉ ch·ªçn tham s·ªë, nh∆∞ng in Test AUC ƒë·ªÉ tham kh·∫£o\n",
    "\n",
    "    torch.cuda.empty_cache()  # X·∫£ VRAM\n",
    "    gc.collect()\n",
    "    return best_val_auc, final_test_auc, best_model\n",
    "\n",
    "\n",
    "\n",
    "# --- PH·∫¶N SCRIPT TEST (TEST BENCH) ---\n",
    "\n",
    "def run_test(data):\n",
    "    l = torch.tensor([is_edge_index_sorted(data[edge_type].edge_index) for edge_type in data.edge_types])\n",
    "    print(l.all())\n",
    "\n",
    "\n",
    "def pre_process_data(data):\n",
    "    # Gi·∫£ s·ª≠ data l√† HeteroData\n",
    "    data.pin_memory()  # TƒÉng t·ªëc transfer d·ªØ li·ªáu\n",
    "    return data\n",
    "\n",
    "\n",
    "def run_optimization(data = None):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Running on: {device}\")\n",
    "\n",
    "    # --- B∆Ø·ªöC 1: CHU·∫®N B·ªä DATA CHO GRID SEARCH ---\n",
    "    print(\"\\n>>> Loading & Splitting Data...\")\n",
    "\n",
    "    # 1. Load d·ªØ li·ªáu g·ªëc\n",
    "    #if data is None:\n",
    "    data = get_or_prepare_data()\n",
    "\n",
    "    # 2. C·∫Øt d·ªØ li·ªáu (In-Place Modification)\n",
    "    # H√†m n√†y s·∫Ω x√≥a c·∫°nh Val/Test kh·ªèi `data` v√† tr·∫£ v·ªÅ indices r·ªùi\n",
    "    # train_graph ch√≠nh l√† bi·∫øn `data` sau khi b·ªã c·∫Øt\n",
    "    split_data = prepare_data_splits(data, val_ratio=0.001, test_ratio=0.001)\n",
    "    config = {\n",
    "        'hidden_dim': 256,\n",
    "        'batch_size': 64,\n",
    "        'lr': 0.01,\n",
    "        'dropout': 0.41372892081262375,\n",
    "        'epochs': 1\n",
    "    }\n",
    "    best_val_auc, test_auc, best_model = train_one_config(split_data, config, device)\n",
    "\n",
    "    repo = ModelRepository(MODEL_PATH)\n",
    "    repo.save_model(best_model)\n",
    "\n",
    "\n",
    "# Loading data\n",
    "#data = ToUndirected(merge=False)(data)\n",
    "#feature.save_data(data,mapping)\n",
    "# Training\n",
    "if __name__ == \"__main__\":\n",
    "    run_optimization()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PHASE 2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from segmentation_models_pytorch.metrics import f1_score\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from infrastructure.repositories import PyGDataRepository, ModelRepository\n",
    "from config.settings import PYG_DATA_PATH, OUTPUT_DIM, TRAINING_HISTORY_PATH, MODEL_PATH\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import gc\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# --- 1. ARCHITECTURE (Gi·ªØ nguy√™n nh∆∞ c≈©) ---\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.ln1 = torch.nn.LayerNorm(hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.ln1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.normalize(x, p=2., dim=-1)\n",
    "        return x\n",
    "\n",
    "class InteractionMLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(input_dim * 3, hidden_dim)\n",
    "        self.lin2 = torch.nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.lin3 = torch.nn.Linear(hidden_dim // 2, output_dim)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, z_src, z_dst):\n",
    "        combined = torch.cat([z_src, z_dst, z_src * z_dst], dim=1)\n",
    "        h = self.lin1(combined)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.lin2(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.lin3(h)\n",
    "        return h.view(-1)\n",
    "\n",
    "class LinkPredictionModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, data=None, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.gnn = GraphSAGE(hidden_channels, out_channels, dropout)\n",
    "        self.encoder = to_hetero(self.gnn, data.metadata(), aggr='sum')\n",
    "        self.decoders = torch.nn.ModuleDict()\n",
    "\n",
    "        unique_rels = set()\n",
    "        for src, rel, dst in data.edge_types:\n",
    "            if not rel.startswith('rev_'):\n",
    "                unique_rels.add(rel)\n",
    "\n",
    "        for rel in unique_rels:\n",
    "            key = f\"__{rel}__\"\n",
    "            self.decoders[key] = InteractionMLP(out_channels, 64, 1, dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, target_edge_type, edge_label_index):\n",
    "        z_dict = self.encoder(x, edge_index)\n",
    "        src_type, rel, dst_type = target_edge_type\n",
    "        z_src = z_dict[src_type][edge_label_index[0]]\n",
    "        z_dst = z_dict[dst_type][edge_label_index[1]]\n",
    "\n",
    "        key = f\"__{rel}__\"\n",
    "        if key in self.decoders:\n",
    "            return self.decoders[key](z_src, z_dst)\n",
    "        else:\n",
    "            return torch.zeros(z_src.size(0), device=z_src.device)\n",
    "\n",
    "# --- 2. DATA UTILS ---\n",
    "\n",
    "def get_or_prepare_data():\n",
    "    feature_repo = PyGDataRepository(PYG_DATA_PATH)\n",
    "    data = feature_repo.load_data()\n",
    "    return data\n",
    "\n",
    "def get_human_hub_edges(data):\n",
    "    \"\"\"\n",
    "    [PHASE 2] L·∫•y c√°c c·∫°nh Human-Hub ƒë·ªÉ Calibration.\n",
    "    \"\"\"\n",
    "    target_edges = []\n",
    "    rev_target_edges = []\n",
    "    # Danh s√°ch Hubs quan tr·ªçng ƒë·ªÉ hi·ªáu ch·ªânh Encoder\n",
    "    calibration_rels = [\n",
    "        ('human', 'educated_at', 'organization'),\n",
    "        ('human', 'educated_at', 'educational_institution'),\n",
    "        ('human','acted_in','film'),\n",
    "        ('human', 'work_at', 'educational_institution'),\n",
    "        ('human', 'work_at', 'organization'),\n",
    "        ('human', 'award_received', 'awards')\n",
    "    ]\n",
    "\n",
    "    print(\"\\n--- SELECTING HUB EDGES FOR CALIBRATION ---\")\n",
    "    for edge_type in data.edge_types:\n",
    "        src, rel, dst = edge_type\n",
    "        if rel.startswith('rev_'):\n",
    "            continue\n",
    "        rev_rel = f\"rev_{rel}\"\n",
    "        if src == 'human' and dst != 'human':\n",
    "            if (src,rel,dst) in calibration_rels:\n",
    "                target_edges.append(edge_type)\n",
    "                rev_target_edges.append((dst, rev_rel, src))\n",
    "                print(f\"Selected: {edge_type}\")\n",
    "\n",
    "\n",
    "    return target_edges, rev_target_edges\n",
    "\n",
    "def prepare_calibration_splits(data):\n",
    "    \"\"\"\n",
    "    Split cho Phase 2: D√πng Human-Hub l√†m Label.\n",
    "    Nh∆∞ng v·∫´n gi·ªØ nguy√™n c·∫•u tr√∫c ƒë·ªì th·ªã.\n",
    "    \"\"\"\n",
    "    target_edges, rev_target_edges = get_human_hub_edges(data)\n",
    "\n",
    "    if len(target_edges) == 0: raise ValueError(\"No hub edges found!\")\n",
    "\n",
    "    transform = T.RandomLinkSplit(\n",
    "        num_val=0.01,  # Ch·ªâ c·∫ßn val √≠t ƒë·ªÉ check trend\n",
    "        num_test=0.01,\n",
    "        is_undirected=False,\n",
    "        edge_types=target_edges,\n",
    "        rev_edge_types=rev_target_edges,\n",
    "        disjoint_train_ratio=0.1, # C·∫Øt 10% Hub ra l√†m b√†i t·∫≠p hi·ªáu ch·ªânh\n",
    "        add_negative_train_samples=False\n",
    "    )\n",
    "\n",
    "    train_data, val_data, test_data = transform(data)\n",
    "    return train_data, val_data, test_data, target_edges\n",
    "\n",
    "def loader_generator(data_source, target_edge_types, batch_size, shuffle=False):\n",
    "    for et in target_edge_types:\n",
    "        if et not in data_source.edge_index_dict: continue\n",
    "        if hasattr(data_source[et], 'edge_label_index') and data_source[et].edge_label_index.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        lbl_index = data_source[et].edge_label_index\n",
    "        lbl_ones = torch.ones(lbl_index.size(1), dtype=torch.float32)\n",
    "\n",
    "        loader = LinkNeighborLoader(\n",
    "            data_source,\n",
    "            num_neighbors=[50, 30],\n",
    "            edge_label_index=(et, lbl_index),\n",
    "            edge_label=lbl_ones,\n",
    "            neg_sampling_ratio=3.0,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=0,\n",
    "            persistent_workers=False\n",
    "        )\n",
    "        yield et, loader\n",
    "\n",
    "# --- 3. TRAIN & EVALUATE ---\n",
    "\n",
    "def train_epoch(model, data, optimizer, device, target_edge_types, scaler, batch_size):\n",
    "    model.train() # ENCODER L√öC N√ÄY ƒêANG M·ªû (UNFROZEN)\n",
    "    total_loss = 0\n",
    "    total_examples = 0\n",
    "\n",
    "    data_loader_gen = loader_generator(data, target_edge_types, batch_size, shuffle=True)\n",
    "\n",
    "    for edge_type, loader in data_loader_gen:\n",
    "        pbar = tqdm(loader, desc=f\"Calibrating {edge_type[1]}\", leave=False)\n",
    "        for batch in pbar:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            edge_label_index = batch[edge_type].edge_label_index\n",
    "            edge_label = batch[edge_type].edge_label\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                # Forward qua Encoder (ƒëang t√≠nh gradient)\n",
    "                out = model(batch.x_dict, batch.edge_index_dict, edge_type, edge_label_index)\n",
    "                loss = F.binary_cross_entropy_with_logits(out, edge_label)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item() * edge_label.size(0)\n",
    "            total_examples += edge_label.size(0)\n",
    "\n",
    "        del loader, pbar\n",
    "\n",
    "    return total_loss / (total_examples + 1e-6)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, device, target_edge_types, batch_size):\n",
    "    model.eval()\n",
    "    preds, ground_truths = [], []\n",
    "\n",
    "    data_loader_gen = loader_generator(data, target_edge_types, batch_size, shuffle=False)\n",
    "    for edge_type, loader in data_loader_gen:\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            if not hasattr(batch[edge_type], 'edge_label_index') or batch[edge_type].edge_label_index.numel() == 0: continue\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                out = model(batch.x_dict, batch.edge_index_dict, edge_type, batch[edge_type].edge_label_index)\n",
    "                out = torch.sigmoid(out)\n",
    "\n",
    "            preds.append(out.cpu().numpy())\n",
    "            ground_truths.append(batch[edge_type].edge_label.cpu().numpy())\n",
    "\n",
    "    if len(preds) == 0: return 0.0\n",
    "    final_labels = np.concatenate(ground_truths)\n",
    "    final_preds = np.concatenate(preds)\n",
    "    ap = 0\n",
    "    f1 = 0\n",
    "    try:\n",
    "        ap = average_precision_score(final_labels, final_preds)\n",
    "        f1 = f1_score(final_labels, (final_preds > 0.5).astype(int))\n",
    "    except:\n",
    "        pass\n",
    "    auc = roc_auc_score(final_labels, final_preds )\n",
    "    return auc, ap, f1\n",
    "\n",
    "def update_training_history(file_path, new_data):\n",
    "    history = []\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                history = json.load(f)\n",
    "                if not isinstance(history, list): history = [history]\n",
    "        except: history = []\n",
    "    if isinstance(new_data, list): history.extend(new_data)\n",
    "    else: history.append(new_data)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(history, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# --- 4. MAIN: PHASE 2 CALIBRATION ---\n",
    "\n",
    "def run_phase2_calibration():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"üöÄ RUNNING PHASE 2: CALIBRATION (UNFROZEN ENCODER) on {device}\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    data = get_or_prepare_data()\n",
    "\n",
    "    # 2. Split Data (D√πng Human-Hub l√†m b√†i t·∫≠p)\n",
    "    train_data, val_data, test_data, target_edges = prepare_calibration_splits(data)\n",
    "\n",
    "    # C·∫•u h√¨nh Phase 2\n",
    "    config = {\n",
    "        'hidden_dim': 256,\n",
    "        'batch_size': 64,\n",
    "        'lr': 1e-5,\n",
    "        'epochs': 1,\n",
    "        'dropout': 0.4\n",
    "    }\n",
    "\n",
    "    # 3. Init Model\n",
    "    print(\"Initializing Model...\", end=\" \")\n",
    "    model = LinkPredictionModel(config['hidden_dim'], OUTPUT_DIM, train_data, config['dropout']).to(device)\n",
    "\n",
    "    # 4. Load Phase 1 Weights\n",
    "    print(\"\\nüì• Loading Phase 1 (Social) Weights...\")\n",
    "    try:\n",
    "        # Load weights, b·ªè qua strict ƒë·ªÉ an to√†n\n",
    "        state_dict = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"Phase 1 Weights Loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading weights: {e}\")\n",
    "\n",
    "    print(\"Encoder is UNFROZEN. Learning Rate set to 1e-5 to preserve knowledge.\")\n",
    "\n",
    "    # L·∫•y param c·ªßa Encoder\n",
    "    params = list(model.encoder.parameters())\n",
    "\n",
    "    added_keys = set()\n",
    "\n",
    "    for src, rel, dst in target_edges:\n",
    "        key = f\"__{rel}__\"\n",
    "        if key in model.decoders and key not in added_keys:\n",
    "            params.extend(list(model.decoders[key].parameters()))\n",
    "            added_keys.add(key) # ƒê√°nh d·∫•u l√† ƒë√£ th√™m\n",
    "\n",
    "    optimizer = torch.optim.Adam(params, lr=config['lr'])\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    # 6. Training Loop\n",
    "    history = {\"epoch\": [], \"loss\": [], \"val_auc\": []}\n",
    "    calib_model_path = str(MODEL_PATH).replace(\".pt\", \"_phase2_calibrated.pt\")\n",
    "    calib_history_path = str(TRAINING_HISTORY_PATH).replace(\".json\", \"_phase2.json\")\n",
    "\n",
    "    print(f\"\\nStart Calibration for {config['epochs']} epochs...\")\n",
    "\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        # Train (Tr√™n c·∫°nh Hubs)\n",
    "        loss = train_epoch(model, train_data, optimizer, device, target_edges, scaler, config['batch_size'])\n",
    "\n",
    "        # Validate (Tr√™n c·∫°nh Hubs - ƒë·ªÉ xem ti·∫øn b·ªô)\n",
    "        val_auc, ap, f1 = evaluate(model, val_data, device, target_edges, config['batch_size'])\n",
    "\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"loss\"].append(loss)\n",
    "        history[\"val_auc\"].append(val_auc)\n",
    "        try:\n",
    "            history[\"ap\"].append(ap)\n",
    "            history[\"f1\"].append(f1)\n",
    "        except:\n",
    "            pass\n",
    "        print(f\"Phase 2 - Epoch {epoch}: Loss={loss:.4f} | Hubs AUC={val_auc:.4f}\")\n",
    "\n",
    "        # Lu√¥n save l·∫°i model sau epoch cu·ªëi (v√¨ ch·ªâ ch·∫°y 1-2 epoch n√™n kh√¥ng c·∫ßn early stop)\n",
    "        repo = ModelRepository(calib_model_path)\n",
    "        repo.save_model(model)\n",
    "    test_auc,ap , f1 = evaluate(model, test_data, device, target_edges, config['batch_size'])\n",
    "    history[\"test_auc\"].append(test_auc)\n",
    "    try:\n",
    "            history[\"test_ap\"].append(ap)\n",
    "            history[\"test_f1\"].append(f1)\n",
    "    except:\n",
    "        pass\n",
    "    print(f\"Phase 2 Completed. Model saved to: {calib_model_path}\")\n",
    "    update_training_history(calib_history_path, history)\n",
    "    return model\n",
    "if __name__ == \"__main__\":\n",
    "    model = run_phase2_calibration()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PHASE 3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T08:34:24.564827140Z",
     "start_time": "2026-01-19T08:30:45.138912224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, to_hetero, HGTConv, Linear\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import roc_auc_score, f1_score,average_precision_score\n",
    "from infrastructure.repositories import PyGDataRepository, ModelRepository\n",
    "from config.settings import PYG_DATA_PATH, OUTPUT_DIM, TRAINING_HISTORY_PATH, MODEL_PATH\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import gc\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# --- 1. ARCHITECTURE CLASSES (Gi·ªØ nguy√™n) ---\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.ln1 = torch.nn.LayerNorm(hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.ln1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.normalize(x, p=2., dim=-1)\n",
    "        return x\n",
    "\n",
    "class InteractionMLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(input_dim * 3, hidden_dim)\n",
    "        self.lin2 = torch.nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.lin3 = torch.nn.Linear(hidden_dim // 2, output_dim)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, z_src, z_dst):\n",
    "        combined = torch.cat([z_src, z_dst, z_src * z_dst], dim=1)\n",
    "        h = self.lin1(combined)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.lin2(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.lin3(h)\n",
    "        return h.view(-1)\n",
    "\n",
    "class LinkPredictionModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, data=None, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.gnn = GraphSAGE(hidden_channels, out_channels, dropout)\n",
    "        self.encoder = to_hetero(self.gnn, data.metadata(), aggr='sum')\n",
    "        self.decoders = torch.nn.ModuleDict()\n",
    "\n",
    "        unique_rels = set()\n",
    "        for src, rel, dst in data.edge_types:\n",
    "            if not rel.startswith('rev_'):\n",
    "                unique_rels.add(rel)\n",
    "\n",
    "        for rel in unique_rels:\n",
    "            key = f\"__{rel}__\"\n",
    "            self.decoders[key] = InteractionMLP(out_channels, 64, 1, dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, target_edge_type, edge_label_index):\n",
    "        # Encoder t√≠nh to√°n ra vector (Embedding)\n",
    "        z_dict = self.encoder(x, edge_index)\n",
    "\n",
    "        src_type, rel, dst_type = target_edge_type\n",
    "        z_src = z_dict[src_type][edge_label_index[0]]\n",
    "        z_dst = z_dict[dst_type][edge_label_index[1]]\n",
    "\n",
    "        key = f\"__{rel}__\"\n",
    "        if key in self.decoders:\n",
    "            return self.decoders[key](z_src, z_dst)\n",
    "        else:\n",
    "            return torch.zeros(z_src.size(0), device=z_src.device)\n",
    "\n",
    "# --- 2. DATA PREPARATION UTILS ---\n",
    "\n",
    "def get_or_prepare_data():\n",
    "    feature_repo = PyGDataRepository(PYG_DATA_PATH)\n",
    "    data = feature_repo.load_data()\n",
    "    if data is None:\n",
    "        print(\"Ch∆∞a c√≥ d·ªØ li·ªáu PyG. Vui l√≤ng ch·∫°y ETL tr∆∞·ªõc!\")\n",
    "        return None\n",
    "    return data\n",
    "\n",
    "def get_recommendation_edges(data):\n",
    "    \"\"\"\n",
    "    [M·ªöI] Ch·ªçn l·ªçc c√°c c·∫°nh Human-Hub ƒë·ªÉ train Decoder.\n",
    "    \"\"\"\n",
    "    target_edges = []\n",
    "    rev_target_edges = []\n",
    "\n",
    "    print(\"\\n--- SELECTING RECOMMENDATION EDGES ---\")\n",
    "    for edge_type in data.edge_types:\n",
    "        src, rel, dst = edge_type\n",
    "        if rel.startswith('rev_'):\n",
    "            continue\n",
    "        rev_rel = f\"rev_{rel}\"\n",
    "        rev_edge_type = (dst, rev_rel, src)\n",
    "        target_edges.append(edge_type)\n",
    "        rev_target_edges.append(rev_edge_type)\n",
    "        print(f\"Selected Target: {edge_type} | Count: {data[edge_type].edge_index.size(1)}\")\n",
    "\n",
    "    return target_edges, rev_target_edges\n",
    "def prepare_decoder_splits(data):\n",
    "    \"\"\"\n",
    "    [M·ªöI] Split d·ªØ li·ªáu chuy√™n d·ª•ng cho Phase 3.\n",
    "    \"\"\"\n",
    "    target_edges, rev_target_edges = get_recommendation_edges(data)\n",
    "\n",
    "    if len(target_edges) == 0:\n",
    "        raise ValueError(\"Kh√¥ng t√¨m th·∫•y c·∫°nh Recommendation n√†o trong d·ªØ li·ªáu!\")\n",
    "\n",
    "    print(\"--- SPLITTING DATA FOR DECODER TUNING ---\")\n",
    "    transform = T.RandomLinkSplit(\n",
    "        num_val=0.05,  # Validation nh·ªè th√¥i\n",
    "        num_test=0.05, # Test nh·ªè th√¥i\n",
    "        is_undirected=False,\n",
    "        edge_types=target_edges,\n",
    "        rev_edge_types=rev_target_edges,\n",
    "\n",
    "        # Ch·ªâ l·∫•y 20% c·∫°nh Hub l√†m \"ƒë·ªÅ thi\", 80% gi·ªØ l·∫°i ƒë·ªÉ Encoder c√≥ ng·ªØ c·∫£nh t·ªët\n",
    "        disjoint_train_ratio=0.2,\n",
    "        add_negative_train_samples=False\n",
    "    )\n",
    "\n",
    "    train_data, val_data, test_data = transform(data)\n",
    "    return train_data, val_data, test_data, target_edges\n",
    "\n",
    "def loader_generator(data_source, target_edge_types, batch_size, shuffle=False):\n",
    "    for et in target_edge_types:\n",
    "        if et not in data_source.edge_index_dict: continue\n",
    "        if hasattr(data_source[et], 'edge_label_index') and data_source[et].edge_label_index.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        lbl_index = data_source[et].edge_label_index\n",
    "        lbl_ones = torch.ones(lbl_index.size(1), dtype=torch.float32)\n",
    "\n",
    "        loader = LinkNeighborLoader(\n",
    "            data_source,\n",
    "            num_neighbors=[50, 30], # Gi·ªØ nguy√™n c·∫•u h√¨nh t·ªët nh·∫•t\n",
    "            edge_label_index=(et, lbl_index),\n",
    "            edge_label=lbl_ones,\n",
    "            neg_sampling_ratio=3.0,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=0,\n",
    "            persistent_workers=False\n",
    "        )\n",
    "        yield et, loader\n",
    "\n",
    "# --- 3. CORE FUNCTIONS (Train & Evaluate) ---\n",
    "\n",
    "def train_epoch(model, data, optimizer, device, target_edge_types, scaler, batch_size):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_examples = 0\n",
    "\n",
    "    data_loader_gen = loader_generator(data, target_edge_types, batch_size, shuffle=True)\n",
    "\n",
    "    for edge_type, loader in data_loader_gen:\n",
    "        pbar = tqdm(loader, desc=f\"Training {edge_type[1]}\", leave=False)\n",
    "\n",
    "        for batch in pbar:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            edge_label_index = batch[edge_type].edge_label_index\n",
    "            edge_label = batch[edge_type].edge_label\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                # Forward (Encoder ƒë√£ ƒë√≥ng bƒÉng s·∫Ω kh√¥ng t√≠nh gradient)\n",
    "                out = model(batch.x_dict, batch.edge_index_dict, edge_type, edge_label_index)\n",
    "                loss = F.binary_cross_entropy_with_logits(out, edge_label)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item() * edge_label.size(0)\n",
    "            total_examples += edge_label.size(0)\n",
    "\n",
    "        del loader, pbar\n",
    "        gc.collect()\n",
    "\n",
    "    return total_loss / (total_examples + 1e-6)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, device, target_edge_types, batch_size):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    ground_truths = []\n",
    "\n",
    "    data_loader_gen = loader_generator(data, target_edge_types, batch_size, shuffle=False)\n",
    "\n",
    "    for edge_type, loader in data_loader_gen:\n",
    "        pbar = tqdm(loader, desc=f\"Validating {edge_type[1]}\", leave=False)\n",
    "        for batch in pbar:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            if not hasattr(batch[edge_type], 'edge_label_index') or batch[edge_type].edge_label_index.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            edge_label_index = batch[edge_type].edge_label_index\n",
    "            edge_label = batch[edge_type].edge_label\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                out = model(batch.x_dict, batch.edge_index_dict, edge_type, edge_label_index)\n",
    "                out = torch.sigmoid(out)\n",
    "\n",
    "            preds.append(out.cpu().numpy())\n",
    "            ground_truths.append(edge_label.cpu().numpy())\n",
    "\n",
    "        del loader, pbar\n",
    "\n",
    "    if len(preds) == 0: return 0.0\n",
    "    final_labels = np.concatenate(ground_truths)\n",
    "    final_preds = np.concatenate(preds)\n",
    "    ap = 0\n",
    "    f1 = 0\n",
    "    try:\n",
    "        ap = average_precision_score(final_labels, final_preds)\n",
    "        f1 = f1_score(final_labels, (final_preds > 0.5).astype(int))\n",
    "    except:\n",
    "        pass\n",
    "    auc = roc_auc_score(final_labels, final_preds )\n",
    "    return auc, ap, f1\n",
    "\n",
    "def update_training_history(file_path, new_data):\n",
    "    history = []\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                history = json.load(f)\n",
    "                if not isinstance(history, list): history = [history]\n",
    "        except: history = []\n",
    "\n",
    "    if isinstance(new_data, list): history.extend(new_data)\n",
    "    else: history.append(new_data)\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(history, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# --- 4. MAIN EXECUTION: DECODER TUNING ---\n",
    "\n",
    "def run_decoder_tuning():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"üöÄ RUNNING PHASE 3: RECOMMENDATION DECODER TUNING on {device}\")\n",
    "    MODEL_PATH = 'data_output/models/model_phase2_calibrated.pt'\n",
    "    # 1. Load Data\n",
    "    data = get_or_prepare_data()\n",
    "\n",
    "    # 2. Split Data (Ch·ªâ quan t√¢m Human-Hub)\n",
    "    train_data, val_data, test_data, target_edges = prepare_decoder_splits(data)\n",
    "\n",
    "    # 3. C·∫•u h√¨nh Phase 3\n",
    "    config = {\n",
    "        'hidden_dim': 256,\n",
    "        'batch_size': 256, # TƒÉng Batch Size l√™n v√¨ Encoder ƒë√£ Freeze (√≠t t·ªën RAM h∆°n)\n",
    "        'lr': 0.01,       # LR cao h∆°n ƒë·ªÉ train MLP nhanh\n",
    "        'epochs': 20,      # Train k·ªπ\n",
    "        'dropout': 0.4\n",
    "    }\n",
    "\n",
    "    # 4. Kh·ªüi t·∫°o Model\n",
    "    print(\"Initializing Model...\", end=\" \")\n",
    "    model = LinkPredictionModel(config['hidden_dim'], OUTPUT_DIM, train_data, config['dropout']).to(device)\n",
    "\n",
    "    # --- QUAN TR·ªåNG: LOAD TR·ªåNG S·ªê T·ª™ PHASE TR∆Ø·ªöC ---\n",
    "    print(\"\\nüì• Loading Pre-trained Encoder weights...\")\n",
    "    try:\n",
    "        # Load weights, b·ªè qua strict ƒë·ªÉ tr√°nh l·ªói n·∫øu shape decoder kh√°c nhau\n",
    "        state_dict = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"‚úÖ Loaded Pre-trained Weights successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Could not load weights. Starting from scratch? Error: {e}\")\n",
    "\n",
    "    # --- QUAN TR·ªåNG: ƒê√ìNG BƒÇNG ENCODER ---\n",
    "    print(\"üîí FREEZING ENCODER (GNN)...\")\n",
    "    for param in model.gnn.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(\"Encoder Frozen. Only Decoders will be updated.\")\n",
    "    params = []\n",
    "    added_keys = set()\n",
    "    for src, rel, dst in target_edges:\n",
    "        key = f\"__{rel}__\"\n",
    "        if key in model.decoders and key not in added_keys:\n",
    "            params.extend(list(model.decoders[key].parameters()))\n",
    "            added_keys.add(key)\n",
    "\n",
    "    if not params:\n",
    "        print(\"‚ùå Error: No decoder parameters found to train!\")\n",
    "        return\n",
    "\n",
    "    optimizer = torch.optim.Adam(params, lr=config['lr'])\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-5)\n",
    "\n",
    "    # 6. Training Loop\n",
    "    history = {\"epoch\": [], \"loss\": [], \"val_auc\": []}\n",
    "    best_val_auc = 0.0\n",
    "    rec_model_path = str(MODEL_PATH).replace(\".pt\", \"_rec_tuned.pth\")\n",
    "    rec_history_path = str(TRAINING_HISTORY_PATH).replace(\".json\", \"_rec.json\")\n",
    "\n",
    "    print(f\"\\nStart Training Decoders for {config['epochs']} epochs...\")\n",
    "\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        # Train\n",
    "        loss = train_epoch(model, train_data, optimizer, device, target_edges, scaler, config['batch_size'])\n",
    "\n",
    "        # Validate\n",
    "        val_auc, ap, f1 = evaluate(model, val_data, device, target_edges, config['batch_size'])\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Logging\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"loss\"].append(loss)\n",
    "        history[\"val_auc\"].append(val_auc)\n",
    "        try:\n",
    "            history[\"ap\"].append(ap)\n",
    "            history[\"f1\"].append(f1)\n",
    "        except:\n",
    "            pass\n",
    "        print(f\"Epoch {epoch:02d} | Loss: {loss:.4f} | Rec Val AUC: {val_auc:.4f} | LR: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "        # Save Best\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            repo = ModelRepository(rec_model_path)\n",
    "            repo.save_model(model)\n",
    "\n",
    "        # Clean VRAM\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        update_training_history(rec_history_path, history)\n",
    "\n",
    "    # 7. Final Test\n",
    "    print(\"\\nüèÅ Evaluating on Test Set...\")\n",
    "    if os.path.exists(rec_model_path):\n",
    "        model.load_state_dict(torch.load(rec_model_path))\n",
    "        test_auc, test_ap, test_f1 = evaluate(model, test_data, device, target_edges, config['batch_size'])\n",
    "        history['test_auc'] = test_auc\n",
    "        try:\n",
    "            history['test_ap'] = test_ap\n",
    "            history['test_f1'] = test_f1\n",
    "        except:\n",
    "            pass\n",
    "        print(f\"FINAL RECOMMENDATION TEST AUC: {test_auc:.4f}\")\n",
    "\n",
    "    update_training_history(rec_history_path, history)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Import l·∫°i c√°c module c·∫ßn thi·∫øt t·ª´ code c≈© c·ªßa b·∫°n\n",
    "# (ƒê·∫£m b·∫£o b·∫°n ƒë√£ import ƒë·ªß c√°c Class: LinkPredictionModel, GraphSAGE, InteractionMLP...)\n",
    "from config.settings import MODEL_PATH, PYG_DATA_PATH, OUTPUT_DIM\n",
    "\n",
    "def get_all_target_edges(data):\n",
    "    \"\"\"Copy l·∫°i h√†m ch·ªçn c·∫°nh b·∫°n ƒë√£ d√πng ·ªü Phase 3\"\"\"\n",
    "    target_edges = []\n",
    "    for edge_type in data.edge_types:\n",
    "        src, rel, dst = edge_type\n",
    "        # L·∫•y t·∫•t c·∫£ c·∫°nh xu√¥i t·ª´ Human (nh∆∞ b·∫°n ƒë√£ c·∫•u h√¨nh Phase 3)\n",
    "        if src == 'human' and not rel.startswith('rev_'):\n",
    "            target_edges.append(edge_type)\n",
    "\n",
    "    rev_target_edges = []\n",
    "    for (src, rel, dst) in target_edges:\n",
    "        rev_rel = f\"rev_{rel}\"\n",
    "        rev_target_edges.append((dst, rev_rel, src))\n",
    "    return target_edges, rev_target_edges\n",
    "\n",
    "def run_final_evaluation():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"üîç STARTING FINAL EVALUATION on {device}\")\n",
    "\n",
    "    # 1. LOAD DATA & SPLIT (Ph·∫£i gi·ªëng h·ªát l√∫c train Phase 3 ƒë·ªÉ Test set ƒë√∫ng)\n",
    "    print(\"1. Loading Data...\")\n",
    "    data = get_or_prepare_data()\n",
    "\n",
    "    # L·∫•y danh s√°ch c·∫°nh ƒë√£ train Phase 3\n",
    "    target_edges, rev_target_edges = get_all_target_edges(data)\n",
    "\n",
    "    print(\"2. Splitting Data (Re-creating Test Set)...\")\n",
    "    # C·∫•u h√¨nh Split gi·ªëng h·ªát Phase 3\n",
    "    transform = T.RandomLinkSplit(\n",
    "        num_val=0.05,\n",
    "        num_test=0.05,\n",
    "        is_undirected=False,\n",
    "        edge_types=target_edges,\n",
    "        rev_edge_types=rev_target_edges,\n",
    "        disjoint_train_ratio=0.2,\n",
    "        add_negative_train_samples=False\n",
    "    )\n",
    "    # Ch√∫ng ta ch·ªâ c·∫ßn test_data\n",
    "    _, _, test_data = transform(data)\n",
    "\n",
    "    # 2. INIT MODEL\n",
    "    print(\"3. Initializing Model Architecture...\")\n",
    "    # L∆∞u √Ω: hidden_dim v√† dropout ph·∫£i kh·ªõp config l√∫c train\n",
    "    model = LinkPredictionModel(hidden_channels=256, out_channels=OUTPUT_DIM, data=test_data, dropout=0.4).to(device)\n",
    "\n",
    "    # 3. LOAD WEIGHTS PHASE 3\n",
    "    # tr·ªè ƒë√∫ng v√†o file model phase 3 m√† b·∫°n ƒë√£ l∆∞u\n",
    "    phase3_path = 'data_output/models/model_phase2_calibrated_rec_tuned.pth'\n",
    "    # Ho·∫∑c n·∫øu b·∫°n l∆∞u ƒë√® l√™n MODEL_PATH th√¨ d√πng MODEL_PATH\n",
    "\n",
    "    print(f\"4. Loading Weights from: {phase3_path}\")\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(phase3_path, map_location=device, weights_only= False))\n",
    "        print(\"‚úÖ Weights Loaded Successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading weights: {e}\")\n",
    "        return\n",
    "\n",
    "    # 4. RUN DETAILED EVALUATION\n",
    "    model.eval()\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"{'RELATIONSHIP':<30} | {'AUC':<8} | {'AP':<8}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Duy·ªát qua t·ª´ng lo·∫°i c·∫°nh ƒë·ªÉ ƒë√°nh gi√° ri√™ng\n",
    "    for et in target_edges:\n",
    "        edge_name = et[1]\n",
    "\n",
    "        # Ki·ªÉm tra xem c√≥ c·∫°nh test kh√¥ng\n",
    "        if (et not in test_data.edge_types) or \\\n",
    "           (not hasattr(test_data[et], 'edge_label_index')) or \\\n",
    "           (test_data[et].edge_label_index.numel() == 0):\n",
    "            continue\n",
    "\n",
    "        # T·∫°o Loader nh·ªè cho Test\n",
    "        lbl_index = test_data[et].edge_label_index\n",
    "        lbl_ones = torch.ones(lbl_index.size(1), dtype=torch.float32)\n",
    "\n",
    "        loader = LinkNeighborLoader(\n",
    "            test_data,\n",
    "            num_neighbors=[50, 30],\n",
    "            edge_label_index=(et, lbl_index),\n",
    "            edge_label=lbl_ones,\n",
    "            neg_sampling_ratio=1.0, # Test 1 th·∫≠t : 1 gi·∫£\n",
    "            batch_size=128,         # Batch an to√†n\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        preds = []\n",
    "        ground_truths = []\n",
    "\n",
    "        for batch in tqdm(loader, desc=f\"Testing {edge_name}\", leave=False):\n",
    "            batch = batch.to(device)\n",
    "            with torch.no_grad(): # T·∫Øt gradient cho nh·∫π\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    out = model(batch.x_dict, batch.edge_index_dict, et, batch[et].edge_label_index)\n",
    "                    scores = torch.sigmoid(out)\n",
    "\n",
    "            preds.append(scores.cpu().numpy())\n",
    "            ground_truths.append(batch[et].edge_label.cpu().numpy())\n",
    "\n",
    "        if len(preds) > 0:\n",
    "            edge_preds = np.concatenate(preds)\n",
    "            edge_labels = np.concatenate(ground_truths)\n",
    "\n",
    "            # T√≠nh ch·ªâ s·ªë\n",
    "            auc = roc_auc_score(edge_labels, edge_preds)\n",
    "            ap = average_precision_score(edge_labels, edge_preds)\n",
    "            f1 = f1_score(edge_labels, (edge_labels > 0.5).astype(int))\n",
    "            print(f\"{edge_name:<30} | {auc:.4f}   | {ap:.4f} | {f1:.4f}\")\n",
    "\n",
    "            all_preds.append(edge_preds)\n",
    "            all_labels.append(edge_labels)\n",
    "        del loader\n",
    "    # 5. T·ªîNG K·∫æT\n",
    "    print(\"=\"*50)\n",
    "    if len(all_preds) > 0:\n",
    "        final_preds = np.concatenate(all_preds)\n",
    "        final_labels = np.concatenate(all_labels)\n",
    "        total_auc = roc_auc_score(final_labels, final_preds)\n",
    "        print(f\"üèÜ OVERALL MODEL AUC: {total_auc:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_final_evaluation()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç STARTING FINAL EVALUATION on cuda\n",
      "1. Loading Data...\n",
      "REPO: ƒêang t·∫£i Processed Data...\n",
      "2. Splitting Data (Re-creating Test Set)...\n",
      "3. Initializing Model Architecture...\n",
      "4. Loading Weights from: data_output/models/model_phase2_calibrated_rec_tuned.pth\n",
      "‚úÖ Weights Loaded Successfully!\n",
      "\n",
      "==================================================\n",
      "RELATIONSHIP                   | AUC      | AP      \n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing acted_in:   0%|          | 0/721 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae9b39ac56d64dd3a9e902184269da1c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acted_in                       | 0.7507   | 0.7977 | 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing active_in:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ceff43a9328f4b8891499bd3850afa44"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active_in                      | 0.7298   | 0.7866 | 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing active_in:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1626db811f7841a38ee3ea502a443353"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active_in                      | 0.7541   | 0.8155 | 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing adheres_to:   0%|          | 0/13 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "280053788061488a938fcca8c6286104"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adheres_to                     | 0.7450   | 0.8061 | 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing advisor_of:   0%|          | 0/231 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "815402f0f0ce4695a010d14bd2ea9276"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T07:34:16.388900525Z",
     "start_time": "2026-01-19T07:34:16.340938590Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T11:00:41.402748093Z",
     "start_time": "2026-01-19T11:00:21.404694977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from config.settings import PYG_DATA_PATH\n",
    "from infrastructure.repositories import PyGDataRepository\n",
    "repo = PyGDataRepository(PYG_DATA_PATH)\n",
    "data = repo.load_data()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO: ƒêang t·∫£i Processed Data...\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T11:01:57.482363741Z",
     "start_time": "2026-01-19T11:01:57.431107665Z"
    }
   },
   "cell_type": "code",
   "source": "spouse = data['human', 'spouse', 'human'].edge_index",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T11:05:00.213299550Z",
     "start_time": "2026-01-19T11:05:00.203542055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src = spouse[0]\n",
    "dst = spouse[1]"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T11:05:26.495888585Z",
     "start_time": "2026-01-19T11:05:24.921806528Z"
    }
   },
   "cell_type": "code",
   "source": "adj = zip(src,dst)",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T11:09:24.633495320Z",
     "start_time": "2026-01-19T11:09:24.584586320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src_id = 1853535\n",
    "dst_id = 278455\n",
    "mask = (spouse[0] == dst_id) & (spouse[1] == src_id)"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T11:09:25.206815796Z",
     "start_time": "2026-01-19T11:09:25.136581194Z"
    }
   },
   "cell_type": "code",
   "source": "mask.any()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for et in data.edge_types:\n",
    "    print(et, data[et].edge_index.size(1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch_geometric.utils import coalesce\n",
    "\n",
    "def remove_duplicate_edges(data):\n",
    "    \"\"\"\n",
    "    Lo·∫°i b·ªè c√°c c·∫°nh tr√πng l·∫∑p trong ƒë·ªì th·ªã (In-place modification).\n",
    "    H·ªó tr·ª£ c·∫£ HeteroData v√† Data chu·∫©n.\n",
    "    \"\"\"\n",
    "    print(\"--- B·∫ÆT ƒê·∫¶U X√ìA C·∫†NH TR√ôNG (Deduplication) ---\")\n",
    "\n",
    "    # 1. X·ª≠ l√Ω cho HeteroData (ƒê·ªì th·ªã d·ªã th·ªÉ)\n",
    "    if hasattr(data, 'edge_types'):\n",
    "        total_removed = 0\n",
    "        for edge_type in data.edge_types:\n",
    "            # L·∫•y edge_index c≈©\n",
    "            edge_index = data[edge_type].edge_index\n",
    "            if edge_index.numel() == 0: continue\n",
    "\n",
    "            old_count = edge_index.size(1)\n",
    "\n",
    "            # L·∫•y s·ªë l∆∞·ª£ng node ƒë·ªÉ t√≠nh to√°n sparse matrix (gi√∫p coalesce nhanh h∆°n)\n",
    "            # src_type, _, dst_type = edge_type\n",
    "            # num_nodes = max(data[src_type].num_nodes, data[dst_type].num_nodes)\n",
    "            # (Ho·∫∑c ƒë·ªÉ None ƒë·ªÉ n√≥ t·ª± t√≠nh max index, h∆°i ch·∫≠m h∆°n x√≠u nh∆∞ng an to√†n)\n",
    "\n",
    "            # TH·∫¶N CH√ö COALESCE\n",
    "            # args: edge_index, edge_attr (None n·∫øu kh√¥ng c√≥), sort_by_row\n",
    "            new_edge_index, _ = coalesce(edge_index, None, sort_by_row=True)\n",
    "\n",
    "            # C·∫≠p nh·∫≠t l·∫°i v√†o data\n",
    "            data[edge_type].edge_index = new_edge_index\n",
    "\n",
    "            new_count = new_edge_index.size(1)\n",
    "            diff = old_count - new_count\n",
    "\n",
    "            if diff > 0:\n",
    "                print(f\"‚úÖ {edge_type}: ƒê√£ x√≥a {diff} c·∫°nh tr√πng ({old_count} -> {new_count})\")\n",
    "                total_removed += diff\n",
    "\n",
    "        if total_removed == 0:\n",
    "            print(\"-> ƒê·ªì th·ªã s·∫°ch, kh√¥ng c√≥ c·∫°nh tr√πng n√†o.\")\n",
    "        else:\n",
    "            print(f\"-> T·ªîNG C·ªòNG ƒê√É X√ìA: {total_removed} c·∫°nh.\")\n",
    "\n",
    "    # 2. X·ª≠ l√Ω cho Data th∆∞·ªùng (Homogeneous)\n",
    "    else:\n",
    "        old_count = data.edge_index.size(1)\n",
    "        data.edge_index, _ = coalesce(data.edge_index, None, sort_by_row=True)\n",
    "        new_count = data.edge_index.size(1)\n",
    "        print(f\"-> ƒê√£ x√≥a {old_count - new_count} c·∫°nh tr√πng.\")\n",
    "\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data = remove_duplicate_edges(data)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch_geometric.transforms import ToUndirected\n",
    "\n",
    "data = ToUndirected(merge=False)(data)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch_geometric.utils import sort_edge_index, coalesce\n",
    "import torch\n",
    "def process_human_edges(data):\n",
    "    print(\"--- Processing Human Edges ---\")\n",
    "\n",
    "    # 1. X√≥a employed_by (Nh∆∞ ƒë√£ b√†n)\n",
    "    if ('human', 'employed_by', 'human') in data.edge_types:\n",
    "        del data['human', 'employed_by', 'human']\n",
    "        print(\"Dropped 'employed_by'\")\n",
    "\n",
    "    # 2. G·ªôp partner v√†o spouse\n",
    "    if ('human', 'partner', 'human') in data.edge_types:\n",
    "        # L·∫•y edges\n",
    "        partner_index = data['human', 'partner', 'human'].edge_index\n",
    "        spouse_index = data['human', 'spouse', 'human'].edge_index\n",
    "\n",
    "        print(f\"Merging: Spouse ({spouse_index.size(1)}) + Partner ({partner_index.size(1)})\")\n",
    "\n",
    "        # A. N·ªëi l·∫°i (Concatenate)\n",
    "        merged_index = torch.cat([spouse_index, partner_index], dim=1)\n",
    "\n",
    "        # B. COALESCE (Th·∫ßn ch√∫ quan tr·ªçng nh·∫•t)\n",
    "        # H√†m n√†y l√†m 2 vi·ªác:\n",
    "        # 1. S·∫Øp x·∫øp l·∫°i (Sort) theo node ngu·ªìn.\n",
    "        # 2. G·ªôp c√°c c·∫°nh tr√πng nhau (Remove Duplicates).\n",
    "        # num_nodes l√† s·ªë l∆∞·ª£ng node Human (c·∫ßn thi·∫øt ƒë·ªÉ t√≠nh to√°n sparse)\n",
    "        num_humans = data['human'].num_nodes\n",
    "        merged_index, _ = coalesce(merged_index, None, num_nodes=num_humans, sort_by_row=True)\n",
    "\n",
    "        # C. C·∫≠p nh·∫≠t l·∫°i v√†o data\n",
    "        data['human', 'spouse', 'human'].edge_index = merged_index\n",
    "\n",
    "        # D. X√≥a partner\n",
    "        del data['human', 'partner', 'human']\n",
    "        print(f\"-> New Spouse size: {merged_index.size(1)}\")\n",
    "\n",
    "    # 3. (Optional) S·∫Øp x·∫øp l·∫°i t·∫•t c·∫£ c√°c lo·∫°i c·∫°nh kh√°c cho ch·∫Øc ƒÉn\n",
    "    # Thao t√°c n√†y r·∫•t nhanh v√† gi√∫p Loader ch·∫°y m∆∞·ª£t h∆°n\n",
    "    if ('human', 'student_of', 'human') in data.edge_types and \\\n",
    "       ('human', 'advisor_of', 'human') in data.edge_types:\n",
    "\n",
    "        # L·∫•y edges\n",
    "        student_index = data['human', 'student_of', 'human'].edge_index\n",
    "        advisor_index = data['human', 'advisor_of', 'human'].edge_index\n",
    "\n",
    "        print(f\"Merging: Advisor ({advisor_index.size(1)}) + Student ({student_index.size(1)})\")\n",
    "\n",
    "        # A. ƒê·∫¢O CHI·ªÄU (FLIP) c·∫°nh student_of\n",
    "        # student_index ƒëang l√† [Row=Student, Col=Teacher]\n",
    "        # Ta l·∫≠t l·∫°i th√†nh [Row=Teacher, Col=Student] ƒë·ªÉ kh·ªõp v·ªõi logic advisor_of\n",
    "        flipped_student_index = torch.stack([student_index[1], student_index[0]], dim=0)\n",
    "\n",
    "        # B. N·ªëi l·∫°i\n",
    "        merged_index = torch.cat([advisor_index, flipped_student_index], dim=1)\n",
    "\n",
    "        # C. COALESCE (S·∫Øp x·∫øp + X√≥a tr√πng)\n",
    "        num_humans = data['human'].num_nodes\n",
    "        merged_index, _ = coalesce(merged_index, None, num_nodes=num_humans, sort_by_row=True)\n",
    "\n",
    "        # D. C·∫≠p nh·∫≠t v√† X√≥a\n",
    "        data['human', 'advisor_of', 'human'].edge_index = merged_index\n",
    "        del data['human', 'student_of', 'human']\n",
    "\n",
    "        print(f\"-> New Advisor size: {merged_index.size(1)}\")\n",
    "\n",
    "    for et in data.edge_types:\n",
    "        data[et].edge_index = sort_edge_index(data[et].edge_index)\n",
    "\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data = process_human_edges(data)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for et in data.edge_types:\n",
    "    if 'rev_' in et[1]:\n",
    "        del data[et]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from core.ai import GraphDataProcessor\n",
    "import pandas as pd\n",
    "nodes = pd.read_parquet('data_output/cleaned/nodes.parquet', engine='pyarrow')\n",
    "edges = pd.read_parquet('data_output/cleaned/edges.parquet', engine='pyarrow')\n",
    "processor = GraphDataProcessor()\n",
    "processor.run(nodes, edges)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading data"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#data = ToUndirected(merge=False)(data)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#feature.save_data(data,mapping)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
